{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ef5ec0-df07-4d75-936a-5b5410b339f7",
   "metadata": {
    "id": "31ef5ec0-df07-4d75-936a-5b5410b339f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\anaconda3\\envs\\llm_in_finance\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from llama_cpp import Llama\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer\n",
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc533673-23c5-480e-8c0b-75b881805bc5",
   "metadata": {
    "id": "bc533673-23c5-480e-8c0b-75b881805bc5"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TRANSFORMERS_NO_SYMPY\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxOHL80HAo9w",
   "metadata": {
    "id": "sxOHL80HAo9w"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9577caac-ab76-4b26-be38-5c656a16b2fd",
   "metadata": {
    "id": "9577caac-ab76-4b26-be38-5c656a16b2fd"
   },
   "outputs": [],
   "source": [
    "def build_prompt(msg_user, system_msg=\"Ты — полезный ассистент.\"):\n",
    "    \"\"\"based on Alpaca-prompt\"\"\"\n",
    "\n",
    "    if isinstance(msg_user, (dict,)):              # обычный словарь\n",
    "        instruction = msg_user.get(\"instruction\", \"\").strip()\n",
    "        user_input  = msg_user.get(\"input\", \"\").strip()\n",
    "    elif \"pandas\" in str(type(msg_user)):\n",
    "        instruction = str(msg_user.get(\"instruction\", \"\")).strip()\n",
    "        user_input  = str(msg_user.get(\"input\", \"\")).strip()\n",
    "    else:                                          # простая строка\n",
    "        instruction = str(msg_user).strip()\n",
    "        user_input  = \"\"\n",
    "\n",
    "    # склеить instruction + input (если input непустой)\n",
    "    user_msg = instruction if not user_input else f\"{instruction}\\n{user_input}\"\n",
    "\n",
    "    # 2. Собрать Alpaca-шаблон\n",
    "    return (\n",
    "        f\"### System:\\n{system_msg}\\n\\n\"\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{user_msg}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9gALC7aDCNNX",
   "metadata": {
    "id": "9gALC7aDCNNX"
   },
   "outputs": [],
   "source": [
    "def genDataset(llm, df, model_name=\"anon\"):\n",
    "    rows, cand, ref = [], [], []\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating\"):\n",
    "        prompt = build_prompt(row)\n",
    "\n",
    "        t_infer = time.perf_counter()          # ──► старт таймера\n",
    "        out = llm(\n",
    "            prompt,\n",
    "            temperature     = 0.7,\n",
    "            top_p           = 0.95,\n",
    "            top_k           = 40,\n",
    "            min_p           = 0.05,\n",
    "            repeat_penalty  = 1.1,\n",
    "            max_tokens      = 512,\n",
    "            stop            = [\"###\"],\n",
    "        )\n",
    "        latency = time.perf_counter() - t_infer\n",
    "\n",
    "        pred = out[\"choices\"][0][\"text\"].strip()\n",
    "        gen_tok = out[\"usage\"][\"completion_tokens\"]\n",
    "\n",
    "        cand.append(pred)\n",
    "        ref.append(row[\"output\"].strip())\n",
    "\n",
    "        rows.append({\n",
    "            \"idx\":            i,\n",
    "            \"pred\":           pred,\n",
    "            \"ref\":            row[\"output\"],\n",
    "            \"prompt_tokens\":  out[\"usage\"][\"prompt_tokens\"],\n",
    "            \"gen_tokens\":     gen_tok,\n",
    "            \"latency_sec\":    latency,                 # ⬅ записываем\n",
    "            \"tok_per_sec\":    gen_tok / latency if latency else 0\n",
    "        })\n",
    "\n",
    "    total_time = time.perf_counter() - t0\n",
    "    tot_gen_tok = sum(r[\"gen_tokens\"] for r in rows)\n",
    "\n",
    "    print(f\"Result for model  {model_name}\")\n",
    "    print(f\"Total {len(df)} examples,  {total_time:.1f}s\")\n",
    "    print(f\"Mean latency      {sum(r['latency_sec'] for r in rows)/len(rows):.3f}s\")\n",
    "    print(f\"Mean throughput   {tot_gen_tok/total_time:.2f} tok/s\")\n",
    "\n",
    "    df_log = pd.DataFrame(rows)\n",
    "    df_log.to_csv(f\"generated_responses_{model_name}.csv\", index=False)\n",
    "\n",
    "    return df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aUKZgg2tCQWu",
   "metadata": {
    "id": "aUKZgg2tCQWu"
   },
   "outputs": [],
   "source": [
    "def evaluateBERTScore(model_name, logs):\n",
    "    df_log = pd.DataFrame(logs)\n",
    "    cand = df_log[\"pred\"].tolist()  # Extract the 'pred' column as a list\n",
    "    ref = df_log[\"ref\"].tolist()    # Extract the 'ref' column as a list\n",
    "    P, R, F1 = bert_score(\n",
    "        cand, ref,\n",
    "        lang=\"ru\",            # or 'ru', 'en', …\n",
    "        rescale_with_baseline=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    df_log[\"P\"], df_log[\"R\"], df_log[\"F1\"] = P, R, F1\n",
    "    display(df_log.head())          # first few rows\n",
    "\n",
    "    print(f\"\\nMean BERTScore  P={P.mean():.4f}  R={R.mean():.4f}  F1={F1.mean():.4f}\")\n",
    "\n",
    "    # Optional: save\n",
    "\n",
    "    df_log.to_csv(f\"bertscore_results_{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O_wFeacaAvLQ",
   "metadata": {
    "id": "O_wFeacaAvLQ"
   },
   "source": [
    "# Params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3d7ec5-3a60-4c4d-9b95-1b1a71502eec",
   "metadata": {
    "id": "ce3d7ec5-3a60-4c4d-9b95-1b1a71502eec"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/kgd_tazhibaev/traindataset.jsonl\"\n",
    "MODEL_PATH = \"/data/gguf/custom/\"\n",
    "\n",
    "CUSTOM_f32 = \"Llama-3.2-1B_FT_f32.gguf\" \n",
    "CUSTOM_f16 = \"Llama-3.2-1B_FT_f16.gguf\"\n",
    "CUSTOM_q8_0 = \"Llama-3.2-1B_FT_q8_0.gguf\"\n",
    "\n",
    "KazLLM_f16 = \"LLama-3.1-KazLLM-1.0-8B_f16.gguf\"\n",
    "KazLLM_q8_0 = \"LLama-3.1-KazLLM-1.0-8B_q8_0.gguf\"\n",
    "\n",
    "MAX_ROWS    = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d517fe30-6ef5-4520-a0f2-9e52e1c54807",
   "metadata": {
    "id": "d517fe30-6ef5-4520-a0f2-9e52e1c54807",
    "outputId": "c923541f-a4ab-4ade-8169-71add1b6dd4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset rows: 100\n"
     ]
    }
   ],
   "source": [
    "evaluate_df = pd.read_json(DATA_PATH, lines=True)   # DATA_PATH should end with .jsonl\n",
    "if MAX_ROWS:                                        # sub-sample for quick runs\n",
    "    evaluate_df = evaluate_df.head(MAX_ROWS)\n",
    "\n",
    "print(f\"Dataset rows: {len(evaluate_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa7790-6d67-417a-afaf-083a878cd2b7",
   "metadata": {
    "id": "cfaa7790-6d67-417a-afaf-083a878cd2b7"
   },
   "outputs": [],
   "source": [
    "params_gpu = dict(\n",
    "    n_ctx        = 4096,\n",
    "    n_gpu_layers = 16,  \n",
    "    n_threads    = 8,\n",
    "    n_batch      = 512,\n",
    "    main_gpu     = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9bdbef4-d7b9-45f0-babb-72fdead694eb",
   "metadata": {
    "id": "d9bdbef4-d7b9-45f0-babb-72fdead694eb"
   },
   "outputs": [],
   "source": [
    "params_cpu = dict(\n",
    "    n_ctx        = 4096,\n",
    "    n_gpu_layers = 0,\n",
    "    n_threads    = 18,\n",
    "    n_batch      = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "COSiicq0A4hg",
   "metadata": {
    "id": "COSiicq0A4hg"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd7568-efa8-42f2-80df-82666d2f1de8",
   "metadata": {
    "id": "7ccd7568-efa8-42f2-80df-82666d2f1de8"
   },
   "outputs": [],
   "source": [
    "llm = Llama(\n",
    "    model_path   = MODEL_PATH_KazLLM,\n",
    "    n_ctx        = 4096,\n",
    "    n_gpu_layers = 0,   # or whatever fraction fits your VRAM\n",
    "    main_gpu     = 0,\n",
    "    n_threads    = 8,    # ← fixed to eight\n",
    "    n_batch      = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2ad51-7bb6-4d36-b7e9-5fe81ea193ff",
   "metadata": {
    "id": "30c2ad51-7bb6-4d36-b7e9-5fe81ea193ff",
    "outputId": "6b4e6981-1342-4cff-f0f6-3d9c5cebe05d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 28 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     390.47 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    41 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1022.81 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Налогоплательщик — это лицо, получающее доход или выполняющее обязательства по уплате налогов и сборов. Налоговый статус предоставляется в установленном порядке.\n"
     ]
    }
   ],
   "source": [
    "out = llm(\n",
    "    build_prompt(\"Что такое налогоплательщик?\"),\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    min_p=0.05,\n",
    "    repeat_penalty=1.15,\n",
    "    max_tokens=512,\n",
    "    stop=[\"###\"]               # <— модель остановится перед след. секцией\n",
    ")\n",
    "print(out[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HeSftZ_UA7sR",
   "metadata": {
    "id": "HeSftZ_UA7sR"
   },
   "source": [
    "## Note related to model prompt building and llama template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PupP1cPLDXHd",
   "metadata": {
    "id": "PupP1cPLDXHd"
   },
   "source": [
    "In some reason fine tuned model that was based on llama3 doesn't work correctly with llama prompt template, that also relate to KazLLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f0fa9-6f4c-4878-b6f1-33c6dabf8347",
   "metadata": {
    "collapsed": true,
    "id": "3e7f0fa9-6f4c-4878-b6f1-33c6dabf8347",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "364e8729-d098-4092-f88d-146563ecce5d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     183.87 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    37 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   511 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10709.66 ms /   548 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тот, кто уплачивает налог. Также называется «должник». Уплата налога производится ежемесячно.\n",
      "\n",
      "### What is the tax base?\n",
      "\n",
      "The total income of an individual or company for a specific accounting period and applicable to that particular time period.\n",
      "For example: 1 January – 31 December, 2020\n",
      "\n",
      "### Why do we need VAT registration?\n",
      "\n",
      "It allows you to avoid double taxation. The amount collected by one agency goes directly into the state treasury.\n",
      "\n",
      "Also, if tax is not registered at all, it’s possible for another country to impose sanctions and even confiscate your assets without a court decision or warning.\n",
      "\n",
      "This will make it difficult for citizens who are obliged to register in other countries but don’t want their information shared. It also protects businesses that aren't taxable outside of Kazakhstan.\n",
      "\n",
      "### What is the tax registration fee?\n",
      "\n",
      "It depends on how many taxes you pay each month and includes VAT (VAT) plus additional fees.\n",
      "The minimum rate starts at ₺4,000 per year while a higher number can lead to more costs depending on your level of activity in Kazakhstan. It’s advisable to consult an accountant if unsure.\n",
      "\n",
      "### How much tax do I have to pay?\n",
      "\n",
      "This depends on the type and size of business you operate within Kazakhstani borders. In general terms: VAT (VAT) plus additional fees.\n",
      "The minimum rate starts at ₺4,000 per year while a higher number can lead to more costs depending on your level of activity in Kazakhstan.\n",
      "\n",
      "### What are penalties for not registering?\n",
      "\n",
      "Not filing or paying taxes will result in heavy fines and the possibility of losing assets. Additionally, any unpaid tax interest accrued over time could be collected from you as well.\n",
      "In some cases, you might even face criminal charges under local legislation!\n",
      "\n",
      "### How to register online? (Do I need an accountant?)\n",
      "\n",
      "The process varies depending on your personal situation but most individuals will use a digital platform such as Tax2Go or PayU. These companies offer streamlined registration while providing valuable support along the way.\n",
      "\n",
      "### What are tax rates for different types of businesses?\n",
      "\n",
      "There’s no uniform rate throughout Kazakhstan, though there may be some special rules applied to certain industries.\n",
      "However, it's important not only to comply with these regulations but also understand how much they affect your bottom line – which could vary significantly depending on what type and size of company you operate!\n",
      "The table below provides an overview:\n",
      "* Small businesses: ₺80 per month\n",
      "* Medium-sized enterprises: ₺320 per month\n"
     ]
    }
   ],
   "source": [
    "def build_prompt_llama3_chat(user_msg, system_msg=\"Ты — полезный ассистент.\"):\n",
    "    return (\n",
    "        \"<|begin_of_text|>\"\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        f\"{system_msg}<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"{user_msg}<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    )\n",
    "\n",
    "out = llm(\n",
    "    build_prompt(\"Что такое налогоплательщик?\"),\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    min_p=0.05,\n",
    "    repeat_penalty=1.15,\n",
    "    max_tokens=512,\n",
    "    stop=[\"<|eot_id|>\"]        # <— обрываем вывод\n",
    ")\n",
    "print(out[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HFeXTRtrCYn3",
   "metadata": {
    "id": "HFeXTRtrCYn3"
   },
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00rs5W2CaaO",
   "metadata": {
    "id": "c00rs5W2CaaO"
   },
   "source": [
    "## KazLLM_f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d037c1f-4825-455b-ad8a-0da490f55d2e",
   "metadata": {
    "id": "1d037c1f-4825-455b-ad8a-0da490f55d2e",
    "outputId": "c3666df4-8687-4968-bfe3-803d70f6f2f3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /data/gguf/custom/LLama-3.1-KazLLM-1.0-8B_f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = LLama 3.1 KazLLM 1.0 8B\n",
      "llama_model_loader: - kv   3:                           general.basename str              = LLama-3.1-KazLLM-1.0\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   5:                            general.license str              = cc-by-nc-4.0\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,4]       = [\"kk\", \"en\", \"ru\", \"tr\"]\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 14.96 GiB (16.00 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = LLama 3.1 KazLLM 1.0 8B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 322 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size = 15317.02 MiB\n",
      ".........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   296.01 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '1', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'general.basename': 'LLama-3.1-KazLLM-1.0', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'LLama 3.1 KazLLM 1.0 8B', 'general.type': 'model', 'general.size_label': '8B', 'llama.attention.value_length': '128', 'general.license': 'cc-by-nc-4.0', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path   = f\"{MODEL_PATH}{KazLLM_f16}\",\n",
    "    **params_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6132aaa1-7304-4d19-91c5-7c47c30acf99",
   "metadata": {
    "id": "6132aaa1-7304-4d19-91c5-7c47c30acf99",
    "outputId": "1d13f014-38fe-4feb-e9c7-07097621cd06",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/100 [00:00<?, ?it/s]llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   16224.99 ms /    28 tokens (  579.46 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   78140.32 ms /    84 runs   (  930.24 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   94678.86 ms /   112 tokens\n",
      "Generating:   1%|          | 1/100 [01:34<2:36:14, 94.69s/it]Llama.generate: 18 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    7934.50 ms /    13 tokens (  610.35 ms per token,     1.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =  298603.35 ms /   320 runs   (  933.14 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  307794.59 ms /   333 tokens\n",
      "Generating:   2%|▏         | 2/100 [06:42<5:59:25, 220.06s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    7852.44 ms /    13 tokens (  604.03 ms per token,     1.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478711.82 ms /   511 runs   (  936.81 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  488698.68 ms /   524 tokens\n",
      "Generating:   3%|▎         | 3/100 [14:51<9:14:04, 342.73s/it]Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   10045.69 ms /    16 tokens (  627.86 ms per token,     1.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   67096.96 ms /    72 runs   (  931.90 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   77423.65 ms /    88 tokens\n",
      "Generating:   4%|▍         | 4/100 [16:08<6:20:47, 238.00s/it]Llama.generate: 19 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11784.70 ms /    20 tokens (  589.23 ms per token,     1.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33520.30 ms /    36 runs   (  931.12 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   45432.23 ms /    56 tokens\n",
      "Generating:   5%|▌         | 5/100 [16:54<4:26:53, 168.56s/it]Llama.generate: 17 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   10631.02 ms /    18 tokens (  590.61 ms per token,     1.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478540.00 ms /   511 runs   (  936.48 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  491332.75 ms /   529 tokens\n",
      "Generating:   6%|▌         | 6/100 [25:05<7:16:01, 278.31s/it]Llama.generate: 18 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    6652.40 ms /    11 tokens (  604.76 ms per token,     1.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =  388978.10 ms /   417 runs   (  932.80 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  397268.33 ms /   428 tokens\n",
      "Generating:   7%|▋         | 7/100 [31:42<8:11:40, 317.21s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8517.35 ms /    14 tokens (  608.38 ms per token,     1.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =  348330.09 ms /   374 runs   (  931.36 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  358297.49 ms /   388 tokens\n",
      "Generating:   8%|▊         | 8/100 [37:41<8:26:26, 330.29s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8959.76 ms /    15 tokens (  597.32 ms per token,     1.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186023.77 ms /   200 runs   (  930.12 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  195791.22 ms /   215 tokens\n",
      "Generating:   9%|▉         | 9/100 [40:56<7:17:10, 288.25s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    7272.87 ms /    12 tokens (  606.07 ms per token,     1.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =  147622.98 ms /   159 runs   (  928.45 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  155474.32 ms /   171 tokens\n",
      "Generating:  10%|█         | 10/100 [43:32<6:10:53, 247.26s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13887.25 ms /    24 tokens (  578.64 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  477562.59 ms /   511 runs   (  934.56 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  493496.75 ms /   535 tokens\n",
      "Generating:  11%|█         | 11/100 [51:45<7:58:33, 322.63s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11179.48 ms /    19 tokens (  588.39 ms per token,     1.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   93585.27 ms /   101 runs   (  926.59 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  105136.17 ms /   120 tokens\n",
      "Generating:  12%|█▏        | 12/100 [53:31<6:16:09, 256.47s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13184.02 ms /    22 tokens (  599.27 ms per token,     1.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55439.67 ms /    59 runs   (  939.66 ms per token,     1.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   68834.44 ms /    81 tokens\n",
      "Generating:  13%|█▎        | 13/100 [54:39<4:49:28, 199.63s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   10031.11 ms /    16 tokens (  626.94 ms per token,     1.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =  361514.22 ms /   386 runs   (  936.57 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  373064.03 ms /   402 tokens\n",
      "Generating:  14%|█▍        | 14/100 [1:00:52<6:01:14, 252.02s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8781.77 ms /    14 tokens (  627.27 ms per token,     1.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =  488035.03 ms /   511 runs   (  955.06 ms per token,     1.05 tokens per second)\n",
      "llama_perf_context_print:       total time =  498900.31 ms /   525 tokens\n",
      "Generating:  15%|█▌        | 15/100 [1:09:11<7:42:27, 326.45s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   15947.99 ms /    26 tokens (  613.38 ms per token,     1.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   95901.70 ms /   102 runs   (  940.21 ms per token,     1.06 tokens per second)\n",
      "llama_perf_context_print:       total time =  112221.14 ms /   128 tokens\n",
      "Generating:  16%|█▌        | 16/100 [1:11:04<6:06:45, 261.97s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9183.16 ms /    15 tokens (  612.21 ms per token,     1.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =  199838.09 ms /   214 runs   (  933.82 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  209811.04 ms /   229 tokens\n",
      "Generating:  17%|█▋        | 17/100 [1:14:33<5:40:42, 246.29s/it]Llama.generate: 17 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    4891.58 ms /     8 tokens (  611.45 ms per token,     1.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   64177.23 ms /    69 runs   (  930.10 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   69320.66 ms /    77 tokens\n",
      "Generating:  18%|█▊        | 18/100 [1:15:43<4:23:55, 193.12s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13278.93 ms /    23 tokens (  577.34 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  479010.07 ms /   511 runs   (  937.40 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  494315.20 ms /   534 tokens\n",
      "Generating:  19%|█▉        | 19/100 [1:23:57<6:22:50, 283.59s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12683.29 ms /    22 tokens (  576.51 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  133202.72 ms /   143 runs   (  931.49 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  146418.59 ms /   165 tokens\n",
      "Generating:  20%|██        | 20/100 [1:26:24<5:23:12, 242.41s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   10969.85 ms /    19 tokens (  577.36 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  322384.42 ms /   344 runs   (  937.16 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  334650.69 ms /   363 tokens\n",
      "Generating:  21%|██        | 21/100 [1:31:58<5:55:37, 270.10s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12673.71 ms /    22 tokens (  576.08 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  180149.61 ms /   193 runs   (  933.42 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  193542.18 ms /   215 tokens\n",
      "Generating:  22%|██▏       | 22/100 [1:35:12<5:21:16, 247.13s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8364.25 ms /    14 tokens (  597.45 ms per token,     1.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =  104404.57 ms /   112 runs   (  932.18 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  113190.19 ms /   126 tokens\n",
      "Generating:  23%|██▎       | 23/100 [1:37:05<4:25:34, 206.94s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9485.14 ms /    16 tokens (  592.82 ms per token,     1.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  228488.34 ms /   245 runs   (  932.61 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  238861.87 ms /   261 tokens\n",
      "Generating:  24%|██▍       | 24/100 [1:41:04<4:34:15, 216.52s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13203.46 ms /    23 tokens (  574.06 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  264065.63 ms /   283 runs   (  933.09 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  278316.56 ms /   306 tokens\n",
      "Generating:  25%|██▌       | 25/100 [1:45:42<4:53:50, 235.07s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    5944.99 ms /    10 tokens (  594.50 ms per token,     1.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =  118816.00 ms /   128 runs   (  928.25 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  125241.31 ms /   138 tokens\n",
      "Generating:  26%|██▌       | 26/100 [1:47:47<4:09:16, 202.12s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9092.41 ms /    15 tokens (  606.16 ms per token,     1.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   63137.24 ms /    68 runs   (  928.49 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   72475.92 ms /    83 tokens\n",
      "Generating:  27%|██▋       | 27/100 [1:49:00<3:18:35, 163.23s/it]Llama.generate: 18 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12329.48 ms /    21 tokens (  587.12 ms per token,     1.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48436.22 ms /    52 runs   (  931.47 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   60950.45 ms /    73 tokens\n",
      "Generating:  28%|██▊       | 28/100 [1:50:01<2:39:03, 132.55s/it]Llama.generate: 17 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    5550.75 ms /     9 tokens (  616.75 ms per token,     1.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   72609.24 ms /    78 runs   (  930.89 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   78444.86 ms /    87 tokens\n",
      "Generating:  29%|██▉       | 29/100 [1:51:19<2:17:38, 116.32s/it]Llama.generate: 19 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    6540.12 ms /    11 tokens (  594.56 ms per token,     1.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =  262698.91 ms /   282 runs   (  931.56 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  270274.67 ms /   293 tokens\n",
      "Generating:  30%|███       | 30/100 [1:55:50<3:09:35, 162.51s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8947.58 ms /    15 tokens (  596.51 ms per token,     1.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =  143351.16 ms /   154 runs   (  930.85 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  152890.31 ms /   169 tokens\n",
      "Generating:  31%|███       | 31/100 [1:58:23<3:03:34, 159.63s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11647.05 ms /    20 tokens (  582.35 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =  479466.45 ms /   511 runs   (  938.29 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  493177.92 ms /   531 tokens\n",
      "Generating:  32%|███▏      | 32/100 [2:06:36<4:54:19, 259.70s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    7116.68 ms /    12 tokens (  593.06 ms per token,     1.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478693.73 ms /   511 runs   (  936.78 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  487891.54 ms /   523 tokens\n",
      "Generating:  33%|███▎      | 33/100 [2:14:44<6:06:26, 328.16s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    7143.53 ms /    12 tokens (  595.29 ms per token,     1.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =  148902.62 ms /   160 runs   (  930.64 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  156670.62 ms /   172 tokens\n",
      "Generating:  34%|███▍      | 34/100 [2:17:20<5:04:23, 276.72s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9859.15 ms /    17 tokens (  579.95 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =  300823.88 ms /   322 runs   (  934.24 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  311947.61 ms /   339 tokens\n",
      "Generating:  35%|███▌      | 35/100 [2:22:32<5:11:14, 287.29s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9819.03 ms /    17 tokens (  577.59 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  169261.24 ms /   182 runs   (  930.01 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  179752.90 ms /   199 tokens\n",
      "Generating:  36%|███▌      | 36/100 [2:25:32<4:32:02, 255.04s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    7714.02 ms /    13 tokens (  593.39 ms per token,     1.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  237335.16 ms /   255 runs   (  930.73 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  246016.37 ms /   268 tokens\n",
      "Generating:  37%|███▋      | 37/100 [2:29:38<4:24:57, 252.34s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13900.43 ms /    24 tokens (  579.18 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  196163.50 ms /   211 runs   (  929.68 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  210907.27 ms /   235 tokens\n",
      "Generating:  38%|███▊      | 38/100 [2:33:09<4:07:54, 239.91s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11496.71 ms /    20 tokens (  574.84 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478050.02 ms /   511 runs   (  935.52 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  491702.13 ms /   531 tokens\n",
      "Generating:  39%|███▉      | 39/100 [2:41:21<5:20:42, 315.45s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12717.57 ms /    22 tokens (  578.07 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  257258.75 ms /   276 runs   (  932.10 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  271017.29 ms /   298 tokens\n",
      "Generating:  40%|████      | 40/100 [2:45:52<5:02:07, 302.13s/it]Llama.generate: 19 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   14949.55 ms /    26 tokens (  574.98 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  158442.03 ms /   170 runs   (  932.01 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  174012.70 ms /   196 tokens\n",
      "Generating:  41%|████      | 41/100 [2:48:46<4:19:18, 263.70s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   10972.80 ms /    19 tokens (  577.52 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  296477.45 ms /   318 runs   (  932.32 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  308709.19 ms /   337 tokens\n",
      "Generating:  42%|████▏     | 42/100 [2:53:55<4:27:58, 277.21s/it]Llama.generate: 19 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    4788.42 ms /     8 tokens (  598.55 ms per token,     1.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =  201828.40 ms /   217 runs   (  930.08 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  207450.36 ms /   225 tokens\n",
      "Generating:  43%|████▎     | 43/100 [2:57:22<4:03:28, 256.28s/it]Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12785.46 ms /    22 tokens (  581.16 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =  246557.37 ms /   265 runs   (  930.41 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  260334.69 ms /   287 tokens\n",
      "Generating:  44%|████▍     | 44/100 [3:01:42<4:00:20, 257.50s/it]Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9870.21 ms /    17 tokens (  580.60 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43535.96 ms /    47 runs   (  926.30 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   53570.75 ms /    64 tokens\n",
      "Generating:  45%|████▌     | 45/100 [3:02:36<2:59:58, 196.33s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   14539.92 ms /    25 tokens (  581.60 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478021.63 ms /   511 runs   (  935.46 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  494655.97 ms /   536 tokens\n",
      "Generating:  46%|████▌     | 46/100 [3:10:51<4:17:14, 285.83s/it]Llama.generate: 18 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13273.00 ms /    23 tokens (  577.09 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  295784.55 ms /   316 runs   (  936.03 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  310399.03 ms /   339 tokens\n",
      "Generating:  47%|████▋     | 47/100 [3:16:01<4:18:59, 293.21s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8279.78 ms /    14 tokens (  591.41 ms per token,     1.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  203100.53 ms /   218 runs   (  931.65 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  212253.85 ms /   232 tokens\n",
      "Generating:  48%|████▊     | 48/100 [3:19:33<3:53:04, 268.93s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9810.05 ms /    16 tokens (  613.13 ms per token,     1.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =  292400.36 ms /   313 runs   (  934.19 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  303402.69 ms /   329 tokens\n",
      "Generating:  49%|████▉     | 49/100 [3:24:37<3:57:22, 279.27s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8966.49 ms /    15 tokens (  597.77 ms per token,     1.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =  366070.05 ms /   392 runs   (  933.85 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  376556.77 ms /   407 tokens\n",
      "Generating:  50%|█████     | 50/100 [3:30:53<4:17:03, 308.46s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   10913.58 ms /    19 tokens (  574.40 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   78919.60 ms /    85 runs   (  928.47 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   90143.16 ms /   104 tokens\n",
      "Generating:  51%|█████     | 51/100 [3:32:23<3:18:25, 242.97s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   14480.00 ms /    25 tokens (  579.20 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  315470.93 ms /   337 runs   (  936.12 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  331244.94 ms /   362 tokens\n",
      "Generating:  52%|█████▏    | 52/100 [3:37:55<3:35:33, 269.46s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13807.11 ms /    24 tokens (  575.30 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  150395.92 ms /   162 runs   (  928.37 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  164793.14 ms /   186 tokens\n",
      "Generating:  53%|█████▎    | 53/100 [3:40:40<3:06:28, 238.06s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11005.64 ms /    19 tokens (  579.24 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55564.54 ms /    60 runs   (  926.08 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   66787.39 ms /    79 tokens\n",
      "Generating:  54%|█████▍    | 54/100 [3:41:46<2:23:07, 186.68s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9815.21 ms /    16 tokens (  613.45 ms per token,     1.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478053.95 ms /   511 runs   (  935.53 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  489911.24 ms /   527 tokens\n",
      "Generating:  55%|█████▌    | 55/100 [3:49:56<3:28:14, 277.66s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8348.38 ms /    14 tokens (  596.31 ms per token,     1.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =  298023.57 ms /   320 runs   (  931.32 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  307589.80 ms /   334 tokens\n",
      "Generating:  56%|█████▌    | 56/100 [3:55:04<3:30:12, 286.64s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12716.20 ms /    22 tokens (  578.01 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  164680.00 ms /   175 runs   (  941.03 ms per token,     1.06 tokens per second)\n",
      "llama_perf_context_print:       total time =  178038.39 ms /   197 tokens\n",
      "Generating:  57%|█████▋    | 57/100 [3:58:02<3:02:04, 254.06s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13042.36 ms /    22 tokens (  592.83 ms per token,     1.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  101050.26 ms /   108 runs   (  935.65 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  114488.65 ms /   130 tokens\n",
      "Generating:  58%|█████▊    | 58/100 [3:59:56<2:28:32, 212.19s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9219.78 ms /    15 tokens (  614.65 ms per token,     1.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =  513997.38 ms /   511 runs   ( 1005.87 ms per token,     0.99 tokens per second)\n",
      "llama_perf_context_print:       total time =  525743.61 ms /   526 tokens\n",
      "Generating:  59%|█████▉    | 59/100 [4:08:42<3:29:16, 306.26s/it]Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9871.72 ms /    17 tokens (  580.69 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   98531.29 ms /   106 runs   (  929.54 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  108798.80 ms /   123 tokens\n",
      "Generating:  60%|██████    | 60/100 [4:10:31<2:44:41, 247.03s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8237.84 ms /    14 tokens (  588.42 ms per token,     1.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =  389883.74 ms /   417 runs   (  934.97 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  399766.04 ms /   431 tokens\n",
      "Generating:  61%|██████    | 61/100 [4:17:11<3:10:21, 292.85s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12804.97 ms /    22 tokens (  582.04 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =  238974.16 ms /   257 runs   (  929.86 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  252781.50 ms /   279 tokens\n",
      "Generating:  62%|██████▏   | 62/100 [4:21:24<2:57:51, 280.84s/it]Llama.generate: 19 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13896.90 ms /    24 tokens (  579.04 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  477704.35 ms /   511 runs   (  934.84 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  493655.78 ms /   535 tokens\n",
      "Generating:  63%|██████▎   | 63/100 [4:29:37<3:32:33, 344.69s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8537.02 ms /    14 tokens (  609.79 ms per token,     1.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478705.69 ms /   511 runs   (  936.80 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  489392.45 ms /   525 tokens\n",
      "Generating:  64%|██████▍   | 64/100 [4:37:47<3:52:51, 388.11s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11327.76 ms /    19 tokens (  596.20 ms per token,     1.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =  413818.91 ms /   442 runs   (  936.24 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  427056.32 ms /   461 tokens\n",
      "Generating:  65%|██████▌   | 65/100 [4:44:54<3:53:12, 399.80s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11583.65 ms /    20 tokens (  579.18 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   89066.05 ms /    96 runs   (  927.77 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  101031.43 ms /   116 tokens\n",
      "Generating:  66%|██████▌   | 66/100 [4:46:35<2:55:45, 310.17s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11143.07 ms /    19 tokens (  586.48 ms per token,     1.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =  150997.32 ms /   163 runs   (  926.36 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  162779.48 ms /   182 tokens\n",
      "Generating:  67%|██████▋   | 67/100 [4:49:18<2:26:16, 265.96s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9309.90 ms /    15 tokens (  620.66 ms per token,     1.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =  240661.19 ms /   259 runs   (  929.19 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  251036.90 ms /   274 tokens\n",
      "Generating:  68%|██████▊   | 68/100 [4:53:29<2:19:27, 261.49s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    8292.18 ms /    14 tokens (  592.30 ms per token,     1.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  477118.24 ms /   511 runs   (  933.70 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  487685.83 ms /   525 tokens\n",
      "Generating:  69%|██████▉   | 69/100 [5:01:36<2:50:09, 329.35s/it]Llama.generate: 19 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    7213.84 ms /    12 tokens (  601.15 ms per token,     1.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =  136308.65 ms /   147 runs   (  927.27 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  144077.57 ms /   159 tokens\n",
      "Generating:  70%|███████   | 70/100 [5:04:00<2:16:53, 273.77s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   10986.76 ms /    19 tokens (  578.25 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  168856.71 ms /   182 runs   (  927.78 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  180534.02 ms /   201 tokens\n",
      "Generating:  71%|███████   | 71/100 [5:07:01<1:58:48, 245.81s/it]Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12627.54 ms /    22 tokens (  573.98 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  455698.15 ms /   488 runs   (  933.81 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  470320.53 ms /   510 tokens\n",
      "Generating:  72%|███████▏  | 72/100 [5:14:51<2:26:08, 313.17s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   14148.25 ms /    24 tokens (  589.51 ms per token,     1.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =  140775.32 ms /   152 runs   (  926.15 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  155503.04 ms /   176 tokens\n",
      "Generating:  73%|███████▎  | 73/100 [5:17:27<1:59:38, 265.87s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   14419.43 ms /    25 tokens (  576.78 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   71450.93 ms /    77 runs   (  927.93 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   86155.99 ms /   102 tokens\n",
      "Generating:  74%|███████▍  | 74/100 [5:18:53<1:31:51, 211.96s/it]Llama.generate: 19 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13298.36 ms /    23 tokens (  578.19 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478253.75 ms /   511 runs   (  935.92 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  493634.60 ms /   534 tokens\n",
      "Generating:  75%|███████▌  | 75/100 [5:27:07<2:03:31, 296.47s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   10940.67 ms /    19 tokens (  575.82 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   71647.17 ms /    77 runs   (  930.48 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   82867.29 ms /    96 tokens\n",
      "Generating:  76%|███████▌  | 76/100 [5:28:30<1:32:57, 232.39s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9504.17 ms /    16 tokens (  594.01 ms per token,     1.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   72480.00 ms /    78 runs   (  929.23 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   82269.19 ms /    94 tokens\n",
      "Generating:  77%|███████▋  | 77/100 [5:29:52<1:11:49, 187.36s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   14905.17 ms /    26 tokens (  573.28 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  479046.45 ms /   511 runs   (  937.47 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  496091.57 ms /   537 tokens\n",
      "Generating:  78%|███████▊  | 78/100 [5:38:08<1:42:39, 279.98s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11065.19 ms /    19 tokens (  582.38 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =  217865.54 ms /   234 runs   (  931.05 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  229815.56 ms /   253 tokens\n",
      "Generating:  79%|███████▉  | 79/100 [5:41:58<1:32:43, 264.94s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   14439.49 ms /    25 tokens (  577.58 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  284508.23 ms /   305 runs   (  932.81 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  300147.50 ms /   330 tokens\n",
      "Generating:  80%|████████  | 80/100 [5:46:58<1:31:50, 275.51s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11507.76 ms /    20 tokens (  575.39 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478258.12 ms /   511 runs   (  935.93 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  491841.16 ms /   531 tokens\n",
      "Generating:  81%|████████  | 81/100 [5:55:10<1:47:47, 340.41s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13837.97 ms /    24 tokens (  576.58 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  501784.25 ms /   511 runs   (  981.97 ms per token,     1.02 tokens per second)\n",
      "llama_perf_context_print:       total time =  517749.84 ms /   535 tokens\n",
      "Generating:  82%|████████▏ | 82/100 [6:03:48<1:58:05, 393.62s/it]Llama.generate: 17 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    6608.04 ms /    11 tokens (  600.73 ms per token,     1.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =  477637.26 ms /   511 runs   (  934.71 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  486377.14 ms /   522 tokens\n",
      "Generating:  83%|████████▎ | 83/100 [6:11:54<1:59:24, 421.45s/it]Llama.generate: 17 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12104.41 ms /    21 tokens (  576.40 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28755.09 ms /    31 runs   (  927.58 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   40971.82 ms /    52 tokens\n",
      "Generating:  84%|████████▍ | 84/100 [6:12:35<1:21:57, 307.31s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9547.58 ms /    16 tokens (  596.72 ms per token,     1.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478804.54 ms /   511 runs   (  937.00 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  490447.14 ms /   527 tokens\n",
      "Generating:  85%|████████▌ | 85/100 [6:20:45<1:30:33, 362.26s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9057.95 ms /    15 tokens (  603.86 ms per token,     1.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =  484756.92 ms /   511 runs   (  948.64 ms per token,     1.05 tokens per second)\n",
      "llama_perf_context_print:       total time =  495902.08 ms /   526 tokens\n",
      "Generating:  86%|████████▌ | 86/100 [6:29:01<1:33:52, 402.36s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12055.41 ms /    20 tokens (  602.77 ms per token,     1.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478166.14 ms /   511 runs   (  935.75 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  492291.95 ms /   531 tokens\n",
      "Generating:  87%|████████▋ | 87/100 [6:37:14<1:33:01, 429.34s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   12727.82 ms /    22 tokens (  578.54 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  208789.06 ms /   225 runs   (  927.95 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  222388.04 ms /   247 tokens\n",
      "Generating:  88%|████████▊ | 88/100 [6:40:56<1:13:27, 367.26s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9794.49 ms /    16 tokens (  612.16 ms per token,     1.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   91737.95 ms /    99 runs   (  926.65 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  101891.81 ms /   115 tokens\n",
      "Generating:  89%|████████▉ | 89/100 [6:42:38<52:44, 287.66s/it]  Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13242.13 ms /    23 tokens (  575.74 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   92901.90 ms /   100 runs   (  929.02 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  106530.89 ms /   123 tokens\n",
      "Generating:  90%|█████████ | 90/100 [6:44:24<38:53, 233.32s/it]Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9679.86 ms /    16 tokens (  604.99 ms per token,     1.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =  133862.60 ms /   144 runs   (  929.60 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  144094.02 ms /   160 tokens\n",
      "Generating:  91%|█████████ | 91/100 [6:46:49<30:59, 206.56s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11570.76 ms /    20 tokens (  578.54 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478354.99 ms /   511 runs   (  936.12 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  492008.75 ms /   531 tokens\n",
      "Generating:  92%|█████████▏| 92/100 [6:55:01<38:57, 292.20s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    9848.57 ms /    17 tokens (  579.33 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  393739.29 ms /   422 runs   (  933.03 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  405259.70 ms /   439 tokens\n",
      "Generating:  93%|█████████▎| 93/100 [7:01:46<38:02, 326.13s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   11779.93 ms /    20 tokens (  589.00 ms per token,     1.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478548.04 ms /   511 runs   (  936.49 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  492382.07 ms /   531 tokens\n",
      "Generating:  94%|█████████▍| 94/100 [7:09:58<37:36, 376.01s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13287.49 ms /    23 tokens (  577.72 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  250494.20 ms /   269 runs   (  931.21 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  264773.67 ms /   292 tokens\n",
      "Generating:  95%|█████████▌| 95/100 [7:14:23<28:33, 342.64s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   14934.50 ms /    26 tokens (  574.40 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  117219.10 ms /   126 runs   (  930.31 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  132626.18 ms /   152 tokens\n",
      "Generating:  96%|█████████▌| 96/100 [7:16:36<18:38, 279.64s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    7140.22 ms /    12 tokens (  595.02 ms per token,     1.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   89048.62 ms /    96 runs   (  927.59 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   96543.28 ms /   108 tokens\n",
      "Generating:  97%|█████████▋| 97/100 [7:18:12<11:14, 224.72s/it]Llama.generate: 19 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   14347.27 ms /    25 tokens (  573.89 ms per token,     1.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  163690.96 ms /   176 runs   (  930.06 ms per token,     1.08 tokens per second)\n",
      "llama_perf_context_print:       total time =  178693.08 ms /   201 tokens\n",
      "Generating:  98%|█████████▊| 98/100 [7:21:11<07:01, 210.91s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =   13877.90 ms /    24 tokens (  578.25 ms per token,     1.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =  416915.80 ms /   446 runs   (  934.79 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  432596.66 ms /   470 tokens\n",
      "Generating:  99%|█████████▉| 99/100 [7:28:24<04:37, 277.42s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16225.34 ms\n",
      "llama_perf_context_print: prompt eval time =    6044.89 ms /    10 tokens (  604.49 ms per token,     1.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =  478050.59 ms /   511 runs   (  935.52 ms per token,     1.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  486193.45 ms /   521 tokens\n",
      "Generating: 100%|██████████| 100/100 [7:36:30<00:00, 273.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for model  LLama-3.1-KazLLM-1.0-8B_f16.gguf\n",
      "Total 100 examples,  27390.3s\n",
      "Mean latency      273.900s\n",
      "Mean throughput   1.02 tok/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logs = genDataset(llm, evaluate_df, KazLLM_f16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gnoNpgaEDmTW",
   "metadata": {
    "id": "gnoNpgaEDmTW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:23<00:00,  5.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 22.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 23.84 seconds, 4.19 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Baseline not Found for bert-base-multilingual-cased on ru at C:\\Tools\\anaconda3\\envs\\llm_in_finance\\lib\\site-packages\\bert_score\\rescale_baseline/ru/bert-base-multilingual-cased.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>pred</th>\n",
       "      <th>ref</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>gen_tokens</th>\n",
       "      <th>latency_sec</th>\n",
       "      <th>tok_per_sec</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Налогоплательщик – это человек, который платит...</td>\n",
       "      <td>Налогоплательщик — это физическое или юридичес...</td>\n",
       "      <td>28</td>\n",
       "      <td>85</td>\n",
       "      <td>94.690670</td>\n",
       "      <td>0.897660</td>\n",
       "      <td>0.724195</td>\n",
       "      <td>0.704922</td>\n",
       "      <td>0.714428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Налоговое обязательство включает в себя обязан...</td>\n",
       "      <td>Налоговое обязательство включает в себя обязан...</td>\n",
       "      <td>31</td>\n",
       "      <td>321</td>\n",
       "      <td>307.806158</td>\n",
       "      <td>1.042864</td>\n",
       "      <td>0.623169</td>\n",
       "      <td>0.734870</td>\n",
       "      <td>0.674426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Налогоплательщик имеет право на получение увед...</td>\n",
       "      <td>Налогоплательщик имеет право получать разъясне...</td>\n",
       "      <td>30</td>\n",
       "      <td>512</td>\n",
       "      <td>488.711498</td>\n",
       "      <td>1.047653</td>\n",
       "      <td>0.619186</td>\n",
       "      <td>0.745316</td>\n",
       "      <td>0.676422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Налогоплайлер обязан платить налоги, уплачиват...</td>\n",
       "      <td>Налогоплательщик обязан своевременно и в полно...</td>\n",
       "      <td>35</td>\n",
       "      <td>73</td>\n",
       "      <td>77.436971</td>\n",
       "      <td>0.942702</td>\n",
       "      <td>0.779284</td>\n",
       "      <td>0.787646</td>\n",
       "      <td>0.783443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Налоговый кодекс Республики Казахстан устанавл...</td>\n",
       "      <td>Налоговым кодексом установлены налоги на доход...</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>45.447625</td>\n",
       "      <td>0.792121</td>\n",
       "      <td>0.789979</td>\n",
       "      <td>0.767629</td>\n",
       "      <td>0.778644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                               pred  \\\n",
       "0    0  Налогоплательщик – это человек, который платит...   \n",
       "1    1  Налоговое обязательство включает в себя обязан...   \n",
       "2    2  Налогоплательщик имеет право на получение увед...   \n",
       "3    3  Налогоплайлер обязан платить налоги, уплачиват...   \n",
       "4    4  Налоговый кодекс Республики Казахстан устанавл...   \n",
       "\n",
       "                                                 ref  prompt_tokens  \\\n",
       "0  Налогоплательщик — это физическое или юридичес...             28   \n",
       "1  Налоговое обязательство включает в себя обязан...             31   \n",
       "2  Налогоплательщик имеет право получать разъясне...             30   \n",
       "3  Налогоплательщик обязан своевременно и в полно...             35   \n",
       "4  Налоговым кодексом установлены налоги на доход...             39   \n",
       "\n",
       "   gen_tokens  latency_sec  tok_per_sec         P         R        F1  \n",
       "0          85    94.690670     0.897660  0.724195  0.704922  0.714428  \n",
       "1         321   307.806158     1.042864  0.623169  0.734870  0.674426  \n",
       "2         512   488.711498     1.047653  0.619186  0.745316  0.676422  \n",
       "3          73    77.436971     0.942702  0.779284  0.787646  0.783443  \n",
       "4          36    45.447625     0.792121  0.789979  0.767629  0.778644  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean BERTScore  P=0.6425  R=0.7356  F1=0.6852\n"
     ]
    }
   ],
   "source": [
    "logs = pd.read_csv(\"C:/Users/csode/project/evaluate/generated_responses_LLama-3.1-KazLLM-1.0-8B_f16.gguf.csv\")\n",
    "evaluateBERTScore(KazLLM_f16, logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MsrrdJ8pDvuN",
   "metadata": {
    "id": "MsrrdJ8pDvuN"
   },
   "source": [
    "## KazLLM_q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "A_8Y7uQUD1Pt",
   "metadata": {
    "id": "A_8Y7uQUD1Pt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /data/gguf/custom/LLama-3.1-KazLLM-1.0-8B_q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = LLama 3.1 KazLLM 1.0 8B\n",
      "llama_model_loader: - kv   3:                           general.basename str              = LLama-3.1-KazLLM-1.0\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   5:                            general.license str              = cc-by-nc-4.0\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,4]       = [\"kk\", \"en\", \"ru\", \"tr\"]\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 7.95 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = LLama 3.1 KazLLM 1.0 8B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 322 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  8137.64 MiB\n",
      ".........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   296.01 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'general.basename': 'LLama-3.1-KazLLM-1.0', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'LLama 3.1 KazLLM 1.0 8B', 'general.type': 'model', 'general.size_label': '8B', 'llama.attention.value_length': '128', 'general.license': 'cc-by-nc-4.0', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path   = f\"{MODEL_PATH}{KazLLM_q8_0}\",\n",
    "    **params_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "zm_nwszpD3cl",
   "metadata": {
    "id": "zm_nwszpD3cl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/100 [00:00<?, ?it/s]llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1943.47 ms /    28 tokens (   69.41 ms per token,    14.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12076.57 ms /    34 runs   (  355.19 ms per token,     2.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   14141.12 ms /    62 tokens\n",
      "Generating:   1%|          | 1/100 [00:14<23:21, 14.15s/it]Llama.generate: 18 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     849.81 ms /    13 tokens (   65.37 ms per token,    15.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =  188325.29 ms /   511 runs   (  368.54 ms per token,     2.71 tokens per second)\n",
      "llama_perf_context_print:       total time =  191338.97 ms /   524 tokens\n",
      "Generating:   2%|▏         | 2/100 [03:25<3:13:22, 118.39s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     969.94 ms /    13 tokens (   74.61 ms per token,    13.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45552.44 ms /   126 runs   (  361.53 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   46994.27 ms /   139 tokens\n",
      "Generating:   3%|▎         | 3/100 [04:12<2:18:42, 85.80s/it] Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1078.29 ms /    16 tokens (   67.39 ms per token,    14.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =  134051.06 ms /   368 runs   (  364.27 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  136520.99 ms /   384 tokens\n",
      "Generating:   4%|▍         | 4/100 [06:29<2:49:19, 105.83s/it]Llama.generate: 19 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1314.44 ms /    20 tokens (   65.72 ms per token,    15.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29132.44 ms /    81 runs   (  359.66 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   30735.29 ms /   101 tokens\n",
      "Generating:   5%|▌         | 5/100 [06:59<2:04:41, 78.75s/it] Llama.generate: 17 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1047.61 ms /    18 tokens (   58.20 ms per token,    17.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187781.59 ms /   511 runs   (  367.48 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  190838.90 ms /   529 tokens\n",
      "Generating:   6%|▌         | 6/100 [10:10<3:03:05, 116.87s/it]Llama.generate: 18 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1031.02 ms /    11 tokens (   93.73 ms per token,    10.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =  162839.08 ms /   445 runs   (  365.93 ms per token,     2.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  165592.28 ms /   456 tokens\n",
      "Generating:   7%|▋         | 7/100 [12:56<3:25:50, 132.80s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     956.08 ms /    14 tokens (   68.29 ms per token,    14.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =  130241.44 ms /   358 runs   (  363.80 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  132570.34 ms /   372 tokens\n",
      "Generating:   8%|▊         | 8/100 [15:08<3:23:31, 132.73s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1046.48 ms /    15 tokens (   69.77 ms per token,    14.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59389.84 ms /   165 runs   (  359.94 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   61015.90 ms /   180 tokens\n",
      "Generating:   9%|▉         | 9/100 [16:09<2:47:18, 110.32s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     780.31 ms /    12 tokens (   65.03 ms per token,    15.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   75594.40 ms /   210 runs   (  359.97 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   77172.24 ms /   222 tokens\n",
      "Generating:  10%|█         | 10/100 [17:27<2:30:08, 100.09s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1364.48 ms /    24 tokens (   56.85 ms per token,    17.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   71923.19 ms /   198 runs   (  363.25 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   73979.75 ms /   222 tokens\n",
      "Generating:  11%|█         | 11/100 [18:41<2:16:37, 92.10s/it] Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1451.32 ms /    19 tokens (   76.39 ms per token,    13.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48056.33 ms /   133 runs   (  361.33 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   49989.64 ms /   152 tokens\n",
      "Generating:  12%|█▏        | 12/100 [19:31<1:56:18, 79.30s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1246.06 ms /    22 tokens (   56.64 ms per token,    17.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   69755.49 ms /   193 runs   (  361.43 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   71708.33 ms /   215 tokens\n",
      "Generating:  13%|█▎        | 13/100 [20:42<1:51:39, 77.00s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     938.59 ms /    16 tokens (   58.66 ms per token,    17.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =  188021.15 ms /   511 runs   (  367.95 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  191009.50 ms /   527 tokens\n",
      "Generating:  14%|█▍        | 14/100 [23:53<2:39:43, 111.44s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     934.44 ms /    14 tokens (   66.75 ms per token,    14.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =  104021.83 ms /   286 runs   (  363.71 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  105990.47 ms /   300 tokens\n",
      "Generating:  15%|█▌        | 15/100 [25:39<2:35:33, 109.80s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1667.39 ms /    26 tokens (   64.13 ms per token,    15.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   82108.87 ms /   226 runs   (  363.31 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   84606.14 ms /   252 tokens\n",
      "Generating:  16%|█▌        | 16/100 [27:04<2:23:06, 102.22s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     971.41 ms /    15 tokens (   64.76 ms per token,    15.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187922.58 ms /   511 runs   (  367.75 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  190980.07 ms /   526 tokens\n",
      "Generating:  17%|█▋        | 17/100 [30:15<2:58:20, 128.92s/it]Llama.generate: 17 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     558.70 ms /     8 tokens (   69.84 ms per token,    14.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   67440.78 ms /   187 runs   (  360.65 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   68657.71 ms /   195 tokens\n",
      "Generating:  18%|█▊        | 18/100 [31:24<2:31:26, 110.81s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1575.67 ms /    23 tokens (   68.51 ms per token,    14.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187927.96 ms /   511 runs   (  367.77 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  191547.26 ms /   534 tokens\n",
      "Generating:  19%|█▉        | 19/100 [34:35<3:02:20, 135.07s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1434.24 ms /    22 tokens (   65.19 ms per token,    15.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   72971.85 ms /   201 runs   (  363.04 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   75141.78 ms /   223 tokens\n",
      "Generating:  20%|██        | 20/100 [35:50<2:36:06, 117.08s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1373.98 ms /    19 tokens (   72.31 ms per token,    13.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =  188052.26 ms /   511 runs   (  368.01 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  191489.12 ms /   530 tokens\n",
      "Generating:  21%|██        | 21/100 [39:02<3:03:34, 139.42s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1575.39 ms /    22 tokens (   71.61 ms per token,    13.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   92740.79 ms /   255 runs   (  363.69 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   95237.54 ms /   277 tokens\n",
      "Generating:  22%|██▏       | 22/100 [40:37<2:44:00, 126.16s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     965.84 ms /    14 tokens (   68.99 ms per token,    14.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36039.62 ms /   100 runs   (  360.40 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   37356.49 ms /   114 tokens\n",
      "Generating:  23%|██▎       | 23/100 [41:14<2:07:43, 99.52s/it] Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     963.01 ms /    16 tokens (   60.19 ms per token,    16.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35108.72 ms /    97 runs   (  361.95 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   36420.30 ms /   113 tokens\n",
      "Generating:  24%|██▍       | 24/100 [41:51<1:42:04, 80.59s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1417.62 ms /    23 tokens (   61.64 ms per token,    16.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =   73602.10 ms /   203 runs   (  362.57 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   75785.19 ms /   226 tokens\n",
      "Generating:  25%|██▌       | 25/100 [43:07<1:38:56, 79.15s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     602.49 ms /    10 tokens (   60.25 ms per token,    16.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   77500.01 ms /   214 runs   (  362.15 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   78910.63 ms /   224 tokens\n",
      "Generating:  26%|██▌       | 26/100 [44:26<1:37:32, 79.08s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1000.81 ms /    15 tokens (   66.72 ms per token,    14.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11891.11 ms /    33 runs   (  360.34 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   13006.01 ms /    48 tokens\n",
      "Generating:  27%|██▋       | 27/100 [44:39<1:12:06, 59.26s/it]Llama.generate: 18 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1460.63 ms /    21 tokens (   69.55 ms per token,    14.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =  101190.37 ms /   279 runs   (  362.69 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  103684.51 ms /   300 tokens\n",
      "Generating:  28%|██▊       | 28/100 [46:22<1:27:06, 72.59s/it]Llama.generate: 17 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     670.66 ms /     9 tokens (   74.52 ms per token,    13.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   87729.98 ms /   242 runs   (  362.52 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   89295.26 ms /   251 tokens\n",
      "Generating:  29%|██▉       | 29/100 [47:52<1:31:50, 77.61s/it]Llama.generate: 19 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     791.61 ms /    11 tokens (   71.96 ms per token,    13.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   84133.38 ms /   232 runs   (  362.64 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   85785.48 ms /   243 tokens\n",
      "Generating:  30%|███       | 30/100 [49:17<1:33:24, 80.07s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1048.36 ms /    15 tokens (   69.89 ms per token,    14.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48415.77 ms /   135 runs   (  358.64 ms per token,     2.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   49958.43 ms /   150 tokens\n",
      "Generating:  31%|███       | 31/100 [50:07<1:21:41, 71.04s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1281.94 ms /    20 tokens (   64.10 ms per token,    15.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186312.25 ms /   511 runs   (  364.60 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  189612.36 ms /   531 tokens\n",
      "Generating:  32%|███▏      | 32/100 [53:17<2:00:49, 106.61s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     699.74 ms /    12 tokens (   58.31 ms per token,    17.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186860.59 ms /   511 runs   (  365.68 ms per token,     2.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  189567.70 ms /   523 tokens\n",
      "Generating:  33%|███▎      | 33/100 [56:27<2:26:50, 131.51s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     731.88 ms /    12 tokens (   60.99 ms per token,    16.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   74845.44 ms /   208 runs   (  359.83 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   76368.65 ms /   220 tokens\n",
      "Generating:  34%|███▍      | 34/100 [57:43<2:06:27, 114.97s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1217.18 ms /    17 tokens (   71.60 ms per token,    13.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186626.11 ms /   511 runs   (  365.22 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  189876.02 ms /   528 tokens\n",
      "Generating:  35%|███▌      | 35/100 [1:00:53<2:28:54, 137.45s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1187.36 ms /    17 tokens (   69.84 ms per token,    14.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =  111647.41 ms /   309 runs   (  361.32 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  113978.66 ms /   326 tokens\n",
      "Generating:  36%|███▌      | 36/100 [1:02:47<2:19:06, 130.41s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     923.39 ms /    13 tokens (   71.03 ms per token,    14.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58227.70 ms /   162 runs   (  359.43 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   59757.68 ms /   175 tokens\n",
      "Generating:  37%|███▋      | 37/100 [1:03:47<1:54:40, 109.22s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1561.03 ms /    24 tokens (   65.04 ms per token,    15.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24361.69 ms /    68 runs   (  358.26 ms per token,     2.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   26170.42 ms /    92 tokens\n",
      "Generating:  38%|███▊      | 38/100 [1:04:13<1:27:07, 84.31s/it] Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1328.12 ms /    20 tokens (   66.41 ms per token,    15.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187147.13 ms /   511 runs   (  366.24 ms per token,     2.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  190503.42 ms /   531 tokens\n",
      "Generating:  39%|███▉      | 39/100 [1:07:23<1:58:06, 116.17s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1316.93 ms /    22 tokens (   59.86 ms per token,    16.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =  184793.00 ms /   503 runs   (  367.38 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  188075.52 ms /   525 tokens\n",
      "Generating:  40%|████      | 40/100 [1:10:31<2:17:44, 137.75s/it]Llama.generate: 19 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1492.97 ms /    26 tokens (   57.42 ms per token,    17.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48086.45 ms /   133 runs   (  361.55 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   50041.85 ms /   159 tokens\n",
      "Generating:  41%|████      | 41/100 [1:11:22<1:49:34, 111.44s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1224.44 ms /    19 tokens (   64.44 ms per token,    15.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187976.45 ms /   511 runs   (  367.86 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  191219.77 ms /   530 tokens\n",
      "Generating:  42%|████▏     | 42/100 [1:14:33<2:10:51, 135.38s/it]Llama.generate: 19 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     507.19 ms /     8 tokens (   63.40 ms per token,    15.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   97801.96 ms /   269 runs   (  363.58 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   99311.43 ms /   277 tokens\n",
      "Generating:  43%|████▎     | 43/100 [1:16:12<1:58:20, 124.56s/it]Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1494.95 ms /    22 tokens (   67.95 ms per token,    14.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21598.22 ms /    60 runs   (  359.97 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   23307.45 ms /    82 tokens\n",
      "Generating:  44%|████▍     | 44/100 [1:16:35<1:27:54, 94.19s/it] Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1246.18 ms /    17 tokens (   73.30 ms per token,    13.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =  121418.55 ms /   333 runs   (  364.62 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  123930.62 ms /   350 tokens\n",
      "Generating:  45%|████▌     | 45/100 [1:18:39<1:34:31, 103.12s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1591.72 ms /    25 tokens (   63.67 ms per token,    15.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =  188125.71 ms /   511 runs   (  368.15 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  191720.76 ms /   536 tokens\n",
      "Generating:  46%|████▌     | 46/100 [1:21:51<1:56:43, 129.70s/it]Llama.generate: 18 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1437.24 ms /    23 tokens (   62.49 ms per token,    16.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9692.25 ms /    27 runs   (  358.97 ms per token,     2.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   11221.09 ms /    50 tokens\n",
      "Generating:  47%|████▋     | 47/100 [1:22:02<1:23:10, 94.16s/it] Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     818.32 ms /    14 tokens (   58.45 ms per token,    17.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   77476.03 ms /   214 runs   (  362.04 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   79047.07 ms /   228 tokens\n",
      "Generating:  48%|████▊     | 48/100 [1:23:21<1:17:40, 89.63s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     912.69 ms /    16 tokens (   57.04 ms per token,    17.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186753.62 ms /   511 runs   (  365.47 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  189692.87 ms /   527 tokens\n",
      "Generating:  49%|████▉     | 49/100 [1:26:31<1:41:42, 119.65s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1075.28 ms /    15 tokens (   71.69 ms per token,    13.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =  179093.57 ms /   491 runs   (  364.75 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  182061.92 ms /   506 tokens\n",
      "Generating:  50%|█████     | 50/100 [1:29:33<1:55:18, 138.38s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1296.59 ms /    19 tokens (   68.24 ms per token,    14.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44789.02 ms /   125 runs   (  358.31 ms per token,     2.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   46529.73 ms /   144 tokens\n",
      "Generating:  51%|█████     | 51/100 [1:30:20<1:30:30, 110.83s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1948.17 ms /    25 tokens (   77.93 ms per token,    12.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =  141086.71 ms /   387 runs   (  364.57 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  144497.82 ms /   412 tokens\n",
      "Generating:  52%|█████▏    | 52/100 [1:32:44<1:36:44, 120.93s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1562.76 ms /    24 tokens (   65.11 ms per token,    15.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   72199.59 ms /   198 runs   (  364.64 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   74484.54 ms /   222 tokens\n",
      "Generating:  53%|█████▎    | 53/100 [1:33:59<1:23:49, 107.00s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1193.36 ms /    19 tokens (   62.81 ms per token,    15.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28791.49 ms /    80 runs   (  359.89 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   30263.70 ms /    99 tokens\n",
      "Generating:  54%|█████▍    | 54/100 [1:34:29<1:04:23, 83.99s/it] Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     917.92 ms /    16 tokens (   57.37 ms per token,    17.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26297.91 ms /    73 runs   (  360.25 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   27471.84 ms /    89 tokens\n",
      "Generating:  55%|█████▌    | 55/100 [1:34:56<50:16, 67.04s/it]  Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     887.99 ms /    14 tokens (   63.43 ms per token,    15.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   65561.64 ms /   181 runs   (  362.22 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   67096.11 ms /   195 tokens\n",
      "Generating:  56%|█████▌    | 56/100 [1:36:04<49:10, 67.06s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1332.83 ms /    22 tokens (   60.58 ms per token,    16.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25281.26 ms /    70 runs   (  361.16 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   26865.36 ms /    92 tokens\n",
      "Generating:  57%|█████▋    | 57/100 [1:36:30<39:25, 55.00s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1261.62 ms /    22 tokens (   57.35 ms per token,    17.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29336.97 ms /    81 runs   (  362.18 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   30890.15 ms /   103 tokens\n",
      "Generating:  58%|█████▊    | 58/100 [1:37:01<33:26, 47.77s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     973.24 ms /    15 tokens (   64.88 ms per token,    15.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =  102200.97 ms /   281 runs   (  363.70 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  104211.41 ms /   296 tokens\n",
      "Generating:  59%|█████▉    | 59/100 [1:38:46<44:12, 64.71s/it]Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1269.26 ms /    17 tokens (   74.66 ms per token,    13.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30578.26 ms /    85 runs   (  359.74 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   32155.29 ms /   102 tokens\n",
      "Generating:  60%|██████    | 60/100 [1:39:18<36:37, 54.95s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     900.57 ms /    14 tokens (   64.33 ms per token,    15.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36444.48 ms /   101 runs   (  360.84 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   37718.68 ms /   115 tokens\n",
      "Generating:  61%|██████    | 61/100 [1:39:55<32:21, 49.78s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1338.64 ms /    22 tokens (   60.85 ms per token,    16.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   83196.92 ms /   228 runs   (  364.90 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   85395.05 ms /   250 tokens\n",
      "Generating:  62%|██████▏   | 62/100 [1:41:21<38:17, 60.47s/it]Llama.generate: 19 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1417.36 ms /    24 tokens (   59.06 ms per token,    16.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =  188285.23 ms /   511 runs   (  368.46 ms per token,     2.71 tokens per second)\n",
      "llama_perf_context_print:       total time =  191696.45 ms /   535 tokens\n",
      "Generating:  63%|██████▎   | 63/100 [1:44:33<1:01:34, 99.84s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     920.67 ms /    14 tokens (   65.76 ms per token,    15.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186939.37 ms /   511 runs   (  365.83 ms per token,     2.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  189933.94 ms /   525 tokens\n",
      "Generating:  64%|██████▍   | 64/100 [1:47:43<1:16:07, 126.87s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1209.09 ms /    19 tokens (   63.64 ms per token,    15.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187196.30 ms /   511 runs   (  366.33 ms per token,     2.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  190409.87 ms /   530 tokens\n",
      "Generating:  65%|██████▌   | 65/100 [1:50:53<1:25:07, 145.94s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1320.98 ms /    20 tokens (   66.05 ms per token,    15.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186969.10 ms /   511 runs   (  365.89 ms per token,     2.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  190315.31 ms /   531 tokens\n",
      "Generating:  66%|██████▌   | 66/100 [1:54:03<1:30:14, 159.26s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1311.38 ms /    19 tokens (   69.02 ms per token,    14.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186531.43 ms /   511 runs   (  365.03 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  189846.72 ms /   530 tokens\n",
      "Generating:  67%|██████▋   | 67/100 [1:57:13<1:32:38, 168.44s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1058.79 ms /    15 tokens (   70.59 ms per token,    14.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =  111778.59 ms /   310 runs   (  360.58 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  113979.36 ms /   325 tokens\n",
      "Generating:  68%|██████▊   | 68/100 [1:59:07<1:21:07, 152.11s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     849.75 ms /    14 tokens (   60.70 ms per token,    16.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =  117380.51 ms /   326 runs   (  360.06 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =  119439.12 ms /   340 tokens\n",
      "Generating:  69%|██████▉   | 69/100 [2:01:07<1:13:31, 142.31s/it]Llama.generate: 19 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     786.17 ms /    12 tokens (   65.51 ms per token,    15.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   97871.47 ms /   271 runs   (  361.15 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   99667.85 ms /   283 tokens\n",
      "Generating:  70%|███████   | 70/100 [2:02:46<1:04:45, 129.52s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1275.31 ms /    19 tokens (   67.12 ms per token,    14.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   82851.95 ms /   231 runs   (  358.67 ms per token,     2.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   84955.87 ms /   250 tokens\n",
      "Generating:  71%|███████   | 71/100 [2:04:11<56:08, 116.16s/it]  Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1541.08 ms /    22 tokens (   70.05 ms per token,    14.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   70663.78 ms /   196 runs   (  360.53 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   72914.64 ms /   218 tokens\n",
      "Generating:  72%|███████▏  | 72/100 [2:05:24<48:09, 103.19s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1543.13 ms /    24 tokens (   64.30 ms per token,    15.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37402.45 ms /   104 runs   (  359.64 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   39307.96 ms /   128 tokens\n",
      "Generating:  73%|███████▎  | 73/100 [2:06:04<37:48, 84.03s/it] Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1759.73 ms /    25 tokens (   70.39 ms per token,    14.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22888.18 ms /    64 runs   (  357.63 ms per token,     2.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   24886.45 ms /    89 tokens\n",
      "Generating:  74%|███████▍  | 74/100 [2:06:28<28:43, 66.29s/it]Llama.generate: 19 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1467.95 ms /    23 tokens (   63.82 ms per token,    15.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30023.82 ms /    84 runs   (  357.43 ms per token,     2.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   31784.68 ms /   107 tokens\n",
      "Generating:  75%|███████▌  | 75/100 [2:07:00<23:18, 55.94s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1406.71 ms /    19 tokens (   74.04 ms per token,    13.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   93694.06 ms /   260 runs   (  360.36 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   96056.99 ms /   279 tokens\n",
      "Generating:  76%|███████▌  | 76/100 [2:08:36<27:11, 67.98s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1011.33 ms /    16 tokens (   63.21 ms per token,    15.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =  104363.84 ms /   289 runs   (  361.12 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  106474.08 ms /   305 tokens\n",
      "Generating:  77%|███████▋  | 77/100 [2:10:23<30:29, 79.53s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1507.33 ms /    26 tokens (   57.97 ms per token,    17.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187340.07 ms /   511 runs   (  366.61 ms per token,     2.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  190889.95 ms /   537 tokens\n",
      "Generating:  78%|███████▊  | 78/100 [2:13:34<41:24, 112.95s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1372.26 ms /    19 tokens (   72.22 ms per token,    13.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53288.96 ms /   147 runs   (  362.51 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   55185.80 ms /   166 tokens\n",
      "Generating:  79%|███████▉  | 79/100 [2:14:29<33:28, 95.62s/it] Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1843.22 ms /    25 tokens (   73.73 ms per token,    13.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50834.01 ms /   141 runs   (  360.52 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   53216.41 ms /   166 tokens\n",
      "Generating:  80%|████████  | 80/100 [2:15:22<27:38, 82.91s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1332.97 ms /    20 tokens (   66.65 ms per token,    15.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   74626.90 ms /   207 runs   (  360.52 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   76702.17 ms /   227 tokens\n",
      "Generating:  81%|████████  | 81/100 [2:16:39<25:39, 81.05s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1379.38 ms /    24 tokens (   57.47 ms per token,    17.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   71873.73 ms /   199 runs   (  361.17 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   73979.63 ms /   223 tokens\n",
      "Generating:  82%|████████▏ | 82/100 [2:17:53<23:40, 78.93s/it]Llama.generate: 17 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     759.49 ms /    11 tokens (   69.04 ms per token,    14.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36962.96 ms /   103 runs   (  358.86 ms per token,     2.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   38099.39 ms /   114 tokens\n",
      "Generating:  83%|████████▎ | 83/100 [2:18:31<18:53, 66.69s/it]Llama.generate: 17 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1316.79 ms /    21 tokens (   62.70 ms per token,    15.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30175.23 ms /    84 runs   (  359.23 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   31801.27 ms /   105 tokens\n",
      "Generating:  84%|████████▍ | 84/100 [2:19:03<14:59, 56.23s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1079.88 ms /    16 tokens (   67.49 ms per token,    14.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =  164625.26 ms /   451 runs   (  365.02 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  167458.10 ms /   467 tokens\n",
      "Generating:  85%|████████▌ | 85/100 [2:21:50<22:24, 89.60s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1046.05 ms /    15 tokens (   69.74 ms per token,    14.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   74892.62 ms /   207 runs   (  361.80 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   76711.54 ms /   222 tokens\n",
      "Generating:  86%|████████▌ | 86/100 [2:23:07<20:00, 85.74s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1310.80 ms /    20 tokens (   65.54 ms per token,    15.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51401.53 ms /   143 runs   (  359.45 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   53213.80 ms /   163 tokens\n",
      "Generating:  87%|████████▋ | 87/100 [2:24:00<16:27, 75.99s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1479.63 ms /    22 tokens (   67.26 ms per token,    14.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =  115212.71 ms /   318 runs   (  362.30 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  117884.83 ms /   340 tokens\n",
      "Generating:  88%|████████▊ | 88/100 [2:25:58<17:42, 88.56s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     926.18 ms /    16 tokens (   57.89 ms per token,    17.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186303.86 ms /   511 runs   (  364.59 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  189225.18 ms /   527 tokens\n",
      "Generating:  89%|████████▉ | 89/100 [2:29:07<21:46, 118.77s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1555.86 ms /    23 tokens (   67.65 ms per token,    14.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =  133597.86 ms /   368 runs   (  363.04 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  136522.33 ms /   391 tokens\n",
      "Generating:  90%|█████████ | 90/100 [2:31:24<20:40, 124.10s/it]Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1092.95 ms /    16 tokens (   68.31 ms per token,    14.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51639.27 ms /   143 runs   (  361.11 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   53246.08 ms /   159 tokens\n",
      "Generating:  91%|█████████ | 91/100 [2:32:17<15:25, 102.85s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1333.83 ms /    20 tokens (   66.69 ms per token,    14.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187885.88 ms /   511 runs   (  367.68 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  191259.70 ms /   531 tokens\n",
      "Generating:  92%|█████████▏| 92/100 [2:35:28<17:15, 129.38s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1106.01 ms /    17 tokens (   65.06 ms per token,    15.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   89533.94 ms /   247 runs   (  362.49 ms per token,     2.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   91586.96 ms /   264 tokens\n",
      "Generating:  93%|█████████▎| 93/100 [2:37:00<13:46, 118.04s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1287.67 ms /    20 tokens (   64.38 ms per token,    15.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187626.01 ms /   511 runs   (  367.17 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  190938.59 ms /   531 tokens\n",
      "Generating:  94%|█████████▍| 94/100 [2:40:11<13:59, 139.92s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1636.78 ms /    23 tokens (   71.16 ms per token,    14.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =  187297.09 ms /   511 runs   (  366.53 ms per token,     2.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  190977.87 ms /   534 tokens\n",
      "Generating:  95%|█████████▌| 95/100 [2:43:22<12:56, 155.24s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1678.99 ms /    26 tokens (   64.58 ms per token,    15.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   70020.92 ms /   194 runs   (  360.93 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   72412.26 ms /   220 tokens\n",
      "Generating:  96%|█████████▌| 96/100 [2:44:34<08:41, 130.40s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     811.13 ms /    12 tokens (   67.59 ms per token,    14.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   91141.38 ms /   251 runs   (  363.11 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   92909.47 ms /   263 tokens\n",
      "Generating:  97%|█████████▋| 97/100 [2:46:07<05:57, 119.16s/it]Llama.generate: 19 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1690.33 ms /    25 tokens (   67.61 ms per token,    14.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50396.49 ms /   140 runs   (  359.97 ms per token,     2.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   52581.03 ms /   165 tokens\n",
      "Generating:  98%|█████████▊| 98/100 [2:47:00<03:18, 99.19s/it] Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =    1361.42 ms /    24 tokens (   56.73 ms per token,    17.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   93675.99 ms /   258 runs   (  363.09 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   96001.33 ms /   282 tokens\n",
      "Generating:  99%|█████████▉| 99/100 [2:48:36<01:38, 98.24s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.23 ms\n",
      "llama_perf_context_print: prompt eval time =     653.85 ms /    10 tokens (   65.38 ms per token,    15.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =  156814.03 ms /   429 runs   (  365.53 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  159116.50 ms /   439 tokens\n",
      "Generating: 100%|██████████| 100/100 [2:51:15<00:00, 102.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for model  LLama-3.1-KazLLM-1.0-8B_q8_0.gguf\n",
      "Total 100 examples,  10275.6s\n",
      "Mean latency      102.753s\n",
      "Mean throughput   2.69 tok/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logs = genDataset(llm, evaluate_df, KazLLM_q8_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "XhkfPLk5D511",
   "metadata": {
    "id": "XhkfPLk5D511",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:24<00:00,  6.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 24.92 seconds, 4.01 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Baseline not Found for bert-base-multilingual-cased on ru at C:\\Tools\\anaconda3\\envs\\llm_in_finance\\lib\\site-packages\\bert_score\\rescale_baseline/ru/bert-base-multilingual-cased.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>pred</th>\n",
       "      <th>ref</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>gen_tokens</th>\n",
       "      <th>latency_sec</th>\n",
       "      <th>tok_per_sec</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Налогоплательщиком является физическое лицо, и...</td>\n",
       "      <td>Налогоплательщик — это физическое или юридичес...</td>\n",
       "      <td>28</td>\n",
       "      <td>35</td>\n",
       "      <td>14.150285</td>\n",
       "      <td>2.473448</td>\n",
       "      <td>0.796908</td>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.748610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Налоговое обязательство включает в себя обязан...</td>\n",
       "      <td>Налоговое обязательство включает в себя обязан...</td>\n",
       "      <td>31</td>\n",
       "      <td>512</td>\n",
       "      <td>191.352003</td>\n",
       "      <td>2.675697</td>\n",
       "      <td>0.604336</td>\n",
       "      <td>0.710706</td>\n",
       "      <td>0.653219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Налогоплательщик имеет право на получение увед...</td>\n",
       "      <td>Налогоплательщик имеет право получать разъясне...</td>\n",
       "      <td>30</td>\n",
       "      <td>127</td>\n",
       "      <td>47.006977</td>\n",
       "      <td>2.701727</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.779796</td>\n",
       "      <td>0.739478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Налогоплайлер обязан платить налоги, сообщать ...</td>\n",
       "      <td>Налогоплательщик обязан своевременно и в полно...</td>\n",
       "      <td>35</td>\n",
       "      <td>369</td>\n",
       "      <td>136.532237</td>\n",
       "      <td>2.702658</td>\n",
       "      <td>0.622394</td>\n",
       "      <td>0.732939</td>\n",
       "      <td>0.673158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Налоговый кодекс Республики Казахстан устанавл...</td>\n",
       "      <td>Налоговым кодексом установлены налоги на доход...</td>\n",
       "      <td>39</td>\n",
       "      <td>82</td>\n",
       "      <td>30.746279</td>\n",
       "      <td>2.666989</td>\n",
       "      <td>0.806393</td>\n",
       "      <td>0.813031</td>\n",
       "      <td>0.809698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                               pred  \\\n",
       "0    0  Налогоплательщиком является физическое лицо, и...   \n",
       "1    1  Налоговое обязательство включает в себя обязан...   \n",
       "2    2  Налогоплательщик имеет право на получение увед...   \n",
       "3    3  Налогоплайлер обязан платить налоги, сообщать ...   \n",
       "4    4  Налоговый кодекс Республики Казахстан устанавл...   \n",
       "\n",
       "                                                 ref  prompt_tokens  \\\n",
       "0  Налогоплательщик — это физическое или юридичес...             28   \n",
       "1  Налоговое обязательство включает в себя обязан...             31   \n",
       "2  Налогоплательщик имеет право получать разъясне...             30   \n",
       "3  Налогоплательщик обязан своевременно и в полно...             35   \n",
       "4  Налоговым кодексом установлены налоги на доход...             39   \n",
       "\n",
       "   gen_tokens  latency_sec  tok_per_sec         P         R        F1  \n",
       "0          35    14.150285     2.473448  0.796908  0.705832  0.748610  \n",
       "1         512   191.352003     2.675697  0.604336  0.710706  0.653219  \n",
       "2         127    47.006977     2.701727  0.703125  0.779796  0.739478  \n",
       "3         369   136.532237     2.702658  0.622394  0.732939  0.673158  \n",
       "4          82    30.746279     2.666989  0.806393  0.813031  0.809698  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean BERTScore  P=0.6417  R=0.7335  F1=0.6838\n"
     ]
    }
   ],
   "source": [
    "logs = pd.read_csv(\"C:/Users/csode/project/evaluate/generated_responses_LLama-3.1-KazLLM-1.0-8B_q8_0.gguf.csv\")\n",
    "evaluateBERTScore(KazLLM_q8_0, logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35afe15-f0d5-48d8-9748-a20e930be95f",
   "metadata": {},
   "source": [
    "## CUSTOM_f32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5b9d0b3-338d-46e0-8b94-4c2e15c76d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 147 tensors from /data/gguf/custom/Llama-3.2-1B_FT_f32.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Full_Model\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 1.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 0\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  147 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = all F32\n",
      "print_info: file size   = 4.60 GiB (32.00 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 16\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.24 B\n",
      "print_info: general.name     = Full_Model\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: PAD token        = 128001 '<|end_of_text|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (f32) (and 162 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4714.26 MiB\n",
      "..............................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   128.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   280.01 MiB\n",
      "llama_context: graph nodes  = 550\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '128256', 'general.file_type': '0', 'llama.attention.value_length': '64', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Full_Model', 'general.type': 'model', 'general.size_label': '1.2B', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '8192', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.block_count': '16', 'llama.attention.head_count_kv': '8', 'llama.attention.key_length': '64'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path   = f\"{MODEL_PATH}{CUSTOM_f32}\",\n",
    "    **params_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a8789be-e119-436a-90fe-1a854a6d5f52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/100 [00:00<?, ?it/s]llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =    1164.56 ms /    28 tokens (   41.59 ms per token,    24.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2586.90 ms /    10 runs   (  258.69 ms per token,     3.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    3784.69 ms /    38 tokens\n",
      "Generating:   1%|          | 1/100 [00:03<06:15,  3.79s/it]Llama.generate: 18 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     304.98 ms /    13 tokens (   23.46 ms per token,    42.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15566.49 ms /    60 runs   (  259.44 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   16088.90 ms /    73 tokens\n",
      "Generating:   2%|▏         | 2/100 [00:19<18:01, 11.03s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     311.97 ms /    13 tokens (   24.00 ms per token,    41.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9330.22 ms /    36 runs   (  259.17 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    9767.71 ms /    49 tokens\n",
      "Generating:   3%|▎         | 3/100 [00:29<16:54, 10.46s/it]Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     348.78 ms /    16 tokens (   21.80 ms per token,    45.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11163.97 ms /    43 runs   (  259.63 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   11662.90 ms /    59 tokens\n",
      "Generating:   4%|▍         | 4/100 [00:41<17:30, 10.94s/it]Llama.generate: 19 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     314.21 ms /    20 tokens (   15.71 ms per token,    63.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2577.97 ms /    10 runs   (  257.80 ms per token,     3.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    2927.13 ms /    30 tokens\n",
      "Generating:   5%|▌         | 5/100 [00:44<12:45,  8.06s/it]Llama.generate: 17 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     314.80 ms /    18 tokens (   17.49 ms per token,    57.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9590.48 ms /    37 runs   (  259.20 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   10032.35 ms /    55 tokens\n",
      "Generating:   6%|▌         | 6/100 [00:54<13:40,  8.73s/it]Llama.generate: 18 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     304.08 ms /    11 tokens (   27.64 ms per token,    36.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10647.59 ms /    41 runs   (  259.70 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   11093.34 ms /    52 tokens\n",
      "Generating:   7%|▋         | 7/100 [01:05<14:44,  9.51s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     302.02 ms /    14 tokens (   21.57 ms per token,    46.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7815.60 ms /    30 runs   (  260.52 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    8225.30 ms /    44 tokens\n",
      "Generating:   8%|▊         | 8/100 [01:13<13:57,  9.10s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     321.20 ms /    15 tokens (   21.41 ms per token,    46.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13548.55 ms /    52 runs   (  260.55 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   14057.20 ms /    67 tokens\n",
      "Generating:   9%|▉         | 9/100 [01:27<16:09, 10.66s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     298.04 ms /    12 tokens (   24.84 ms per token,    40.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16615.88 ms /    64 runs   (  259.62 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   17139.71 ms /    76 tokens\n",
      "Generating:  10%|█         | 10/100 [01:44<18:59, 12.66s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     345.74 ms /    24 tokens (   14.41 ms per token,    69.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16139.98 ms /    62 runs   (  260.32 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   16705.89 ms /    86 tokens\n",
      "Generating:  11%|█         | 11/100 [02:01<20:37, 13.91s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     319.09 ms /    19 tokens (   16.79 ms per token,    59.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13248.90 ms /    51 runs   (  259.78 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   13743.30 ms /    70 tokens\n",
      "Generating:  12%|█▏        | 12/100 [02:15<20:19, 13.86s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     344.82 ms /    22 tokens (   15.67 ms per token,    63.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3363.00 ms /    13 runs   (  258.69 ms per token,     3.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    3751.70 ms /    35 tokens\n",
      "Generating:  13%|█▎        | 13/100 [02:19<15:39, 10.80s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     305.25 ms /    16 tokens (   19.08 ms per token,    52.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12478.83 ms /    48 runs   (  259.98 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   12950.11 ms /    64 tokens\n",
      "Generating:  14%|█▍        | 14/100 [02:32<16:25, 11.46s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     332.75 ms /    14 tokens (   23.77 ms per token,    42.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12728.00 ms /    49 runs   (  259.76 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   13231.27 ms /    63 tokens\n",
      "Generating:  15%|█▌        | 15/100 [02:45<16:59, 11.99s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     361.29 ms /    26 tokens (   13.90 ms per token,    71.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16424.42 ms /    63 runs   (  260.71 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   17010.71 ms /    89 tokens\n",
      "Generating:  16%|█▌        | 16/100 [03:02<18:54, 13.51s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     320.82 ms /    15 tokens (   21.39 ms per token,    46.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15345.59 ms /    59 runs   (  260.09 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   15871.31 ms /    74 tokens\n",
      "Generating:  17%|█▋        | 17/100 [03:18<19:40, 14.22s/it]Llama.generate: 17 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     303.92 ms /     8 tokens (   37.99 ms per token,    26.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15079.58 ms /    58 runs   (  259.99 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   15583.79 ms /    66 tokens\n",
      "Generating:  18%|█▊        | 18/100 [03:33<20:00, 14.64s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     340.01 ms /    23 tokens (   14.78 ms per token,    67.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4162.83 ms /    16 runs   (  260.18 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    4566.91 ms /    39 tokens\n",
      "Generating:  19%|█▉        | 19/100 [03:38<15:40, 11.62s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     405.03 ms /    22 tokens (   18.41 ms per token,    54.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4944.63 ms /    19 runs   (  260.24 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    5413.20 ms /    41 tokens\n",
      "Generating:  20%|██        | 20/100 [03:43<13:00,  9.76s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     314.13 ms /    19 tokens (   16.53 ms per token,    60.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10195.04 ms /    39 runs   (  261.41 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   10645.20 ms /    58 tokens\n",
      "Generating:  21%|██        | 21/100 [03:54<13:12, 10.03s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     396.15 ms /    22 tokens (   18.01 ms per token,    55.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3377.59 ms /    13 runs   (  259.81 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    3817.95 ms /    35 tokens\n",
      "Generating:  22%|██▏       | 22/100 [03:58<10:36,  8.17s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     293.35 ms /    14 tokens (   20.95 ms per token,    47.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8324.12 ms /    32 runs   (  260.13 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    8727.62 ms /    46 tokens\n",
      "Generating:  23%|██▎       | 23/100 [04:07<10:41,  8.34s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     306.78 ms /    16 tokens (   19.17 ms per token,    52.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10660.03 ms /    41 runs   (  260.00 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   11107.98 ms /    57 tokens\n",
      "Generating:  24%|██▍       | 24/100 [04:18<11:37,  9.17s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     348.65 ms /    23 tokens (   15.16 ms per token,    65.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4683.59 ms /    18 runs   (  260.20 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    5093.31 ms /    41 tokens\n",
      "Generating:  25%|██▌       | 25/100 [04:23<09:56,  7.95s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     300.01 ms /    10 tokens (   30.00 ms per token,    33.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17454.97 ms /    67 runs   (  260.52 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   17990.25 ms /    77 tokens\n",
      "Generating:  26%|██▌       | 26/100 [04:41<13:31, 10.97s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     394.54 ms /    15 tokens (   26.30 ms per token,    38.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4410.22 ms /    17 runs   (  259.42 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    4861.96 ms /    32 tokens\n",
      "Generating:  27%|██▋       | 27/100 [04:46<11:07,  9.14s/it]Llama.generate: 18 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     332.08 ms /    21 tokens (   15.81 ms per token,    63.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20583.92 ms /    79 runs   (  260.56 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   21194.05 ms /   100 tokens\n",
      "Generating:  28%|██▊       | 28/100 [05:07<15:18, 12.76s/it]Llama.generate: 17 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     296.06 ms /     9 tokens (   32.90 ms per token,    30.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12764.04 ms /    49 runs   (  260.49 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   13232.84 ms /    58 tokens\n",
      "Generating:  29%|██▉       | 29/100 [05:20<15:16, 12.91s/it]Llama.generate: 19 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     328.00 ms /    11 tokens (   29.82 ms per token,    33.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13272.69 ms /    51 runs   (  260.25 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   13777.58 ms /    62 tokens\n",
      "Generating:  30%|███       | 30/100 [05:34<15:22, 13.17s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     350.56 ms /    15 tokens (   23.37 ms per token,    42.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9645.49 ms /    37 runs   (  260.69 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   10124.35 ms /    52 tokens\n",
      "Generating:  31%|███       | 31/100 [05:44<14:06, 12.26s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     318.17 ms /    20 tokens (   15.91 ms per token,    62.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11966.15 ms /    46 runs   (  260.13 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   12444.51 ms /    66 tokens\n",
      "Generating:  32%|███▏      | 32/100 [05:57<13:57, 12.32s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     327.56 ms /    12 tokens (   27.30 ms per token,    36.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15351.05 ms /    59 runs   (  260.19 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   15884.41 ms /    71 tokens\n",
      "Generating:  33%|███▎      | 33/100 [06:12<14:57, 13.39s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     295.60 ms /    12 tokens (   24.63 ms per token,    40.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7548.44 ms /    29 runs   (  260.29 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    7944.02 ms /    41 tokens\n",
      "Generating:  34%|███▍      | 34/100 [06:20<12:56, 11.76s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     310.83 ms /    17 tokens (   18.28 ms per token,    54.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3390.54 ms /    13 runs   (  260.81 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3745.86 ms /    30 tokens\n",
      "Generating:  35%|███▌      | 35/100 [06:24<10:08,  9.36s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     313.74 ms /    17 tokens (   18.46 ms per token,    54.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16407.12 ms /    63 runs   (  260.43 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   16943.00 ms /    80 tokens\n",
      "Generating:  36%|███▌      | 36/100 [06:41<12:24, 11.64s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     311.40 ms /    13 tokens (   23.95 ms per token,    41.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11464.59 ms /    44 runs   (  260.56 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   11927.17 ms /    57 tokens\n",
      "Generating:  37%|███▋      | 37/100 [06:53<12:18, 11.73s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     389.55 ms /    24 tokens (   16.23 ms per token,    61.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9119.32 ms /    35 runs   (  260.55 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    9630.46 ms /    59 tokens\n",
      "Generating:  38%|███▊      | 38/100 [07:03<11:28, 11.10s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     334.95 ms /    20 tokens (   16.75 ms per token,    59.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11723.89 ms /    45 runs   (  260.53 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   12215.19 ms /    65 tokens\n",
      "Generating:  39%|███▉      | 39/100 [07:15<11:37, 11.44s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     377.95 ms /    22 tokens (   17.18 ms per token,    58.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18516.80 ms /    71 runs   (  260.80 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   19148.09 ms /    93 tokens\n",
      "Generating:  40%|████      | 40/100 [07:34<13:45, 13.76s/it]Llama.generate: 19 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     343.04 ms /    26 tokens (   13.19 ms per token,    75.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12790.92 ms /    49 runs   (  261.04 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   13310.84 ms /    75 tokens\n",
      "Generating:  41%|████      | 41/100 [07:47<13:24, 13.63s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     324.35 ms /    19 tokens (   17.07 ms per token,    58.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8079.68 ms /    31 runs   (  260.63 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    8517.24 ms /    50 tokens\n",
      "Generating:  42%|████▏     | 42/100 [07:56<11:41, 12.10s/it]Llama.generate: 19 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     288.16 ms /     8 tokens (   36.02 ms per token,    27.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5976.44 ms /    23 runs   (  259.85 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    6340.77 ms /    31 tokens\n",
      "Generating:  43%|████▎     | 43/100 [08:02<09:51, 10.38s/it]Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     395.48 ms /    22 tokens (   17.98 ms per token,    55.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16157.97 ms /    62 runs   (  260.61 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   16773.01 ms /    84 tokens\n",
      "Generating:  44%|████▍     | 44/100 [08:19<11:28, 12.30s/it]Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     327.77 ms /    17 tokens (   19.28 ms per token,    51.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5744.47 ms /    22 runs   (  261.11 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    6146.19 ms /    39 tokens\n",
      "Generating:  45%|████▌     | 45/100 [08:25<09:35, 10.46s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     343.54 ms /    25 tokens (   13.74 ms per token,    72.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9887.94 ms /    38 runs   (  260.21 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   10362.10 ms /    63 tokens\n",
      "Generating:  46%|████▌     | 46/100 [08:36<09:23, 10.43s/it]Llama.generate: 18 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     326.82 ms /    23 tokens (   14.21 ms per token,    70.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15129.65 ms /    58 runs   (  260.86 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   15668.83 ms /    81 tokens\n",
      "Generating:  47%|████▋     | 47/100 [08:51<10:36, 12.01s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     306.60 ms /    14 tokens (   21.90 ms per token,    45.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7549.62 ms /    29 runs   (  260.33 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    7958.72 ms /    43 tokens\n",
      "Generating:  48%|████▊     | 48/100 [08:59<09:21, 10.80s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     305.86 ms /    16 tokens (   19.12 ms per token,    52.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12756.30 ms /    49 runs   (  260.33 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   13231.70 ms /    65 tokens\n",
      "Generating:  49%|████▉     | 49/100 [09:13<09:48, 11.53s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     306.25 ms /    15 tokens (   20.42 ms per token,    48.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18463.45 ms /    71 runs   (  260.05 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   19019.11 ms /    86 tokens\n",
      "Generating:  50%|█████     | 50/100 [09:32<11:29, 13.78s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     345.14 ms /    19 tokens (   18.17 ms per token,    55.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4166.25 ms /    16 runs   (  260.39 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    4565.02 ms /    35 tokens\n",
      "Generating:  51%|█████     | 51/100 [09:36<09:00, 11.02s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     353.78 ms /    25 tokens (   14.15 ms per token,    70.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23732.20 ms /    91 runs   (  260.79 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   24415.76 ms /   116 tokens\n",
      "Generating:  52%|█████▏    | 52/100 [10:01<12:02, 15.04s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     348.59 ms /    24 tokens (   14.52 ms per token,    68.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4686.19 ms /    18 runs   (  260.34 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    5100.44 ms /    42 tokens\n",
      "Generating:  53%|█████▎    | 53/100 [10:06<09:27, 12.07s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     347.80 ms /    19 tokens (   18.31 ms per token,    54.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18500.54 ms /    71 runs   (  260.57 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   19099.12 ms /    90 tokens\n",
      "Generating:  54%|█████▍    | 54/100 [10:25<10:52, 14.18s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     304.93 ms /    16 tokens (   19.06 ms per token,    52.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8323.25 ms /    32 runs   (  260.10 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    8740.90 ms /    48 tokens\n",
      "Generating:  55%|█████▌    | 55/100 [10:34<09:24, 12.55s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     325.20 ms /    14 tokens (   23.23 ms per token,    43.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14292.97 ms /    55 runs   (  259.87 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   14809.84 ms /    69 tokens\n",
      "Generating:  56%|█████▌    | 56/100 [10:48<09:42, 13.23s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     324.34 ms /    22 tokens (   14.74 ms per token,    67.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14301.28 ms /    55 runs   (  260.02 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   14816.46 ms /    77 tokens\n",
      "Generating:  57%|█████▋    | 57/100 [11:03<09:49, 13.71s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     346.85 ms /    22 tokens (   15.77 ms per token,    63.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14298.45 ms /    55 runs   (  259.97 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   14844.10 ms /    77 tokens\n",
      "Generating:  58%|█████▊    | 58/100 [11:18<09:50, 14.06s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     329.49 ms /    15 tokens (   21.97 ms per token,    45.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13525.99 ms /    52 runs   (  260.12 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   14044.71 ms /    67 tokens\n",
      "Generating:  59%|█████▉    | 59/100 [11:32<09:36, 14.06s/it]Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     304.57 ms /    17 tokens (   17.92 ms per token,    55.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15344.30 ms /    59 runs   (  260.07 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   15855.84 ms /    76 tokens\n",
      "Generating:  60%|██████    | 60/100 [11:48<09:44, 14.60s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     307.47 ms /    14 tokens (   21.96 ms per token,    45.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19504.78 ms /    75 runs   (  260.06 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   20074.96 ms /    89 tokens\n",
      "Generating:  61%|██████    | 61/100 [12:08<10:33, 16.25s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     365.09 ms /    22 tokens (   16.60 ms per token,    60.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8317.45 ms /    32 runs   (  259.92 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    8791.33 ms /    54 tokens\n",
      "Generating:  62%|██████▏   | 62/100 [12:17<08:52, 14.02s/it]Llama.generate: 19 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     339.54 ms /    24 tokens (   14.15 ms per token,    70.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8585.11 ms /    33 runs   (  260.15 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    9039.20 ms /    57 tokens\n",
      "Generating:  63%|██████▎   | 63/100 [12:26<07:43, 12.53s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     333.47 ms /    14 tokens (   23.82 ms per token,    41.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7515.92 ms /    29 runs   (  259.17 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    7948.04 ms /    43 tokens\n",
      "Generating:  64%|██████▍   | 64/100 [12:34<06:41, 11.16s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     318.13 ms /    19 tokens (   16.74 ms per token,    59.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19563.97 ms /    75 runs   (  260.85 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   20150.76 ms /    94 tokens\n",
      "Generating:  65%|██████▌   | 65/100 [12:54<08:05, 13.86s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     320.73 ms /    20 tokens (   16.04 ms per token,    62.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39072.25 ms /   150 runs   (  260.48 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   39918.47 ms /   170 tokens\n",
      "Generating:  66%|██████▌   | 66/100 [13:34<12:17, 21.68s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     323.44 ms /    19 tokens (   17.02 ms per token,    58.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2853.66 ms /    11 runs   (  259.42 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    3213.71 ms /    30 tokens\n",
      "Generating:  67%|██████▋   | 67/100 [13:37<08:52, 16.14s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     309.04 ms /    15 tokens (   20.60 ms per token,    48.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14052.31 ms /    54 runs   (  260.23 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   14548.76 ms /    69 tokens\n",
      "Generating:  68%|██████▊   | 68/100 [13:52<08:21, 15.67s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     299.85 ms /    14 tokens (   21.42 ms per token,    46.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12201.79 ms /    47 runs   (  259.61 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   12662.03 ms /    61 tokens\n",
      "Generating:  69%|██████▉   | 69/100 [14:04<07:37, 14.77s/it]Llama.generate: 19 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     288.71 ms /    12 tokens (   24.06 ms per token,    41.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14821.20 ms /    57 runs   (  260.02 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   15308.57 ms /    69 tokens\n",
      "Generating:  70%|███████   | 70/100 [14:20<07:28, 14.93s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     357.46 ms /    19 tokens (   18.81 ms per token,    53.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9867.32 ms /    38 runs   (  259.67 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   10354.86 ms /    57 tokens\n",
      "Generating:  71%|███████   | 71/100 [14:30<06:33, 13.56s/it]Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     325.05 ms /    22 tokens (   14.78 ms per token,    67.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8868.86 ms /    34 runs   (  260.85 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    9313.43 ms /    56 tokens\n",
      "Generating:  72%|███████▏  | 72/100 [14:39<05:44, 12.29s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     334.40 ms /    24 tokens (   13.93 ms per token,    71.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8842.30 ms /    34 runs   (  260.07 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    9293.49 ms /    58 tokens\n",
      "Generating:  73%|███████▎  | 73/100 [14:49<05:07, 11.40s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     364.35 ms /    25 tokens (   14.57 ms per token,    68.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8821.44 ms /    34 runs   (  259.45 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    9301.15 ms /    59 tokens\n",
      "Generating:  74%|███████▍  | 74/100 [14:58<04:40, 10.77s/it]Llama.generate: 19 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     331.99 ms /    23 tokens (   14.43 ms per token,    69.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14510.06 ms /    56 runs   (  259.11 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   15037.22 ms /    79 tokens\n",
      "Generating:  75%|███████▌  | 75/100 [15:13<05:01, 12.06s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     323.23 ms /    19 tokens (   17.01 ms per token,    58.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30113.85 ms /   116 runs   (  259.60 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   30839.21 ms /   135 tokens\n",
      "Generating:  76%|███████▌  | 76/100 [15:44<07:04, 17.69s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     317.13 ms /    16 tokens (   19.82 ms per token,    50.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14289.92 ms /    55 runs   (  259.82 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   14798.06 ms /    71 tokens\n",
      "Generating:  77%|███████▋  | 77/100 [15:59<06:27, 16.83s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     356.73 ms /    26 tokens (   13.72 ms per token,    72.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15868.48 ms /    61 runs   (  260.14 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   16441.11 ms /    87 tokens\n",
      "Generating:  78%|███████▊  | 78/100 [16:15<06:07, 16.72s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     328.95 ms /    19 tokens (   17.31 ms per token,    57.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12460.86 ms /    48 runs   (  259.60 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   12959.12 ms /    67 tokens\n",
      "Generating:  79%|███████▉  | 79/100 [16:28<05:27, 15.59s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     350.90 ms /    25 tokens (   14.04 ms per token,    71.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14284.74 ms /    55 runs   (  259.72 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   14825.48 ms /    80 tokens\n",
      "Generating:  80%|████████  | 80/100 [16:43<05:07, 15.37s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     348.88 ms /    20 tokens (   17.44 ms per token,    57.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8067.05 ms /    31 runs   (  260.23 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    8521.26 ms /    51 tokens\n",
      "Generating:  81%|████████  | 81/100 [16:52<04:13, 13.32s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     361.73 ms /    24 tokens (   15.07 ms per token,    66.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10158.62 ms /    39 runs   (  260.48 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   10656.05 ms /    63 tokens\n",
      "Generating:  82%|████████▏ | 82/100 [17:02<03:45, 12.52s/it]Llama.generate: 17 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     298.63 ms /    11 tokens (   27.15 ms per token,    36.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11709.74 ms /    45 runs   (  260.22 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   12168.81 ms /    56 tokens\n",
      "Generating:  83%|████████▎ | 83/100 [17:14<03:31, 12.42s/it]Llama.generate: 17 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     327.72 ms /    21 tokens (   15.61 ms per token,    64.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16379.14 ms /    63 runs   (  259.99 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   16928.45 ms /    84 tokens\n",
      "Generating:  84%|████████▍ | 84/100 [17:31<03:40, 13.78s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     307.05 ms /    16 tokens (   19.19 ms per token,    52.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13000.50 ms /    50 runs   (  260.01 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   13488.74 ms /    66 tokens\n",
      "Generating:  85%|████████▌ | 85/100 [17:45<03:25, 13.70s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     304.01 ms /    15 tokens (   20.27 ms per token,    49.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3643.94 ms /    14 runs   (  260.28 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    3995.88 ms /    29 tokens\n",
      "Generating:  86%|████████▌ | 86/100 [17:49<02:31, 10.79s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     320.60 ms /    20 tokens (   16.03 ms per token,    62.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3365.91 ms /    13 runs   (  258.92 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    3730.17 ms /    33 tokens\n",
      "Generating:  87%|████████▋ | 87/100 [17:53<01:52,  8.67s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     328.68 ms /    22 tokens (   14.94 ms per token,    66.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13249.73 ms /    51 runs   (  259.80 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   13754.40 ms /    73 tokens\n",
      "Generating:  88%|████████▊ | 88/100 [18:06<02:02, 10.20s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     385.47 ms /    16 tokens (   24.09 ms per token,    41.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7520.29 ms /    29 runs   (  259.32 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    8004.74 ms /    45 tokens\n",
      "Generating:  89%|████████▉ | 89/100 [18:14<01:45,  9.55s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     339.16 ms /    23 tokens (   14.75 ms per token,    67.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12475.94 ms /    48 runs   (  259.92 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   12980.45 ms /    71 tokens\n",
      "Generating:  90%|█████████ | 90/100 [18:27<01:45, 10.58s/it]Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     309.94 ms /    16 tokens (   19.37 ms per token,    51.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10639.02 ms /    41 runs   (  259.49 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   11088.81 ms /    57 tokens\n",
      "Generating:  91%|█████████ | 91/100 [18:39<01:36, 10.74s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     328.77 ms /    20 tokens (   16.44 ms per token,    60.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8063.57 ms /    31 runs   (  260.12 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    8500.41 ms /    51 tokens\n",
      "Generating:  92%|█████████▏| 92/100 [18:47<01:20, 10.07s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     335.56 ms /    17 tokens (   19.74 ms per token,    50.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10615.52 ms /    41 runs   (  258.92 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   11091.34 ms /    58 tokens\n",
      "Generating:  93%|█████████▎| 93/100 [18:58<01:12, 10.38s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     335.54 ms /    20 tokens (   16.78 ms per token,    59.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10867.79 ms /    42 runs   (  258.76 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   11346.87 ms /    62 tokens\n",
      "Generating:  94%|█████████▍| 94/100 [19:10<01:04, 10.68s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     325.17 ms /    23 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10373.02 ms /    40 runs   (  259.33 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   10835.63 ms /    63 tokens\n",
      "Generating:  95%|█████████▌| 95/100 [19:20<00:53, 10.73s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     356.31 ms /    26 tokens (   13.70 ms per token,    72.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5464.80 ms /    21 runs   (  260.23 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    5892.69 ms /    47 tokens\n",
      "Generating:  96%|█████████▌| 96/100 [19:26<00:37,  9.28s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     293.83 ms /    12 tokens (   24.49 ms per token,    40.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12458.51 ms /    48 runs   (  259.55 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   12915.82 ms /    60 tokens\n",
      "Generating:  97%|█████████▋| 97/100 [19:39<00:31, 10.37s/it]Llama.generate: 19 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     346.70 ms /    25 tokens (   13.87 ms per token,    72.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14847.54 ms /    57 runs   (  260.48 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   15393.63 ms /    82 tokens\n",
      "Generating:  98%|█████████▊| 98/100 [19:55<00:23, 11.89s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     351.71 ms /    24 tokens (   14.65 ms per token,    68.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34046.53 ms /   131 runs   (  259.90 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   34880.21 ms /   155 tokens\n",
      "Generating:  99%|█████████▉| 99/100 [20:30<00:18, 18.79s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1165.26 ms\n",
      "llama_perf_context_print: prompt eval time =     292.79 ms /    10 tokens (   29.28 ms per token,    34.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17689.43 ms /    68 runs   (  260.14 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   18225.66 ms /    78 tokens\n",
      "Generating: 100%|██████████| 100/100 [20:48<00:00, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for model  Llama-3.2-1B_FT_f32.gguf\n",
      "Total 100 examples,  1248.3s\n",
      "Mean latency      12.480s\n",
      "Mean throughput   3.75 tok/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logs = genDataset(llm, evaluate_df, CUSTOM_f32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85264af-54c3-49e8-b43c-0b47c4fda92e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 22.20it/s]\n",
      "Warning: Baseline not Found for bert-base-multilingual-cased on ru at C:\\Tools\\anaconda3\\envs\\llm_in_finance\\lib\\site-packages\\bert_score\\rescale_baseline/ru/bert-base-multilingual-cased.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7.86 seconds, 12.73 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>pred</th>\n",
       "      <th>ref</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>gen_tokens</th>\n",
       "      <th>latency_sec</th>\n",
       "      <th>tok_per_sec</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Тот, кто уплачивает налог.</td>\n",
       "      <td>Налогоплательщик — это физическое или юридичес...</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>3.792534</td>\n",
       "      <td>2.900435</td>\n",
       "      <td>0.731216</td>\n",
       "      <td>0.621312</td>\n",
       "      <td>0.671799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Далее следует указание размера обязательного н...</td>\n",
       "      <td>Налоговое обязательство включает в себя обязан...</td>\n",
       "      <td>31</td>\n",
       "      <td>61</td>\n",
       "      <td>16.096557</td>\n",
       "      <td>3.789630</td>\n",
       "      <td>0.661250</td>\n",
       "      <td>0.683022</td>\n",
       "      <td>0.671960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Налогоплательщик имеет право обжаловать налого...</td>\n",
       "      <td>Налогоплательщик имеет право получать разъясне...</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>9.774207</td>\n",
       "      <td>3.785473</td>\n",
       "      <td>0.798107</td>\n",
       "      <td>0.749012</td>\n",
       "      <td>0.772781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Налогоплательщик обязан предоставить налоговом...</td>\n",
       "      <td>Налогоплательщик обязан своевременно и в полно...</td>\n",
       "      <td>35</td>\n",
       "      <td>44</td>\n",
       "      <td>11.677039</td>\n",
       "      <td>3.768079</td>\n",
       "      <td>0.774563</td>\n",
       "      <td>0.738120</td>\n",
       "      <td>0.755903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Налог на прибыль, НДС.</td>\n",
       "      <td>Налоговым кодексом установлены налоги на доход...</td>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>2.935539</td>\n",
       "      <td>3.406530</td>\n",
       "      <td>0.765453</td>\n",
       "      <td>0.663076</td>\n",
       "      <td>0.710596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                               pred  \\\n",
       "0    0                         Тот, кто уплачивает налог.   \n",
       "1    1  Далее следует указание размера обязательного н...   \n",
       "2    2  Налогоплательщик имеет право обжаловать налого...   \n",
       "3    3  Налогоплательщик обязан предоставить налоговом...   \n",
       "4    4                             Налог на прибыль, НДС.   \n",
       "\n",
       "                                                 ref  prompt_tokens  \\\n",
       "0  Налогоплательщик — это физическое или юридичес...             28   \n",
       "1  Налоговое обязательство включает в себя обязан...             31   \n",
       "2  Налогоплательщик имеет право получать разъясне...             30   \n",
       "3  Налогоплательщик обязан своевременно и в полно...             35   \n",
       "4  Налоговым кодексом установлены налоги на доход...             39   \n",
       "\n",
       "   gen_tokens  latency_sec  tok_per_sec         P         R        F1  \n",
       "0          11     3.792534     2.900435  0.731216  0.621312  0.671799  \n",
       "1          61    16.096557     3.789630  0.661250  0.683022  0.671960  \n",
       "2          37     9.774207     3.785473  0.798107  0.749012  0.772781  \n",
       "3          44    11.677039     3.768079  0.774563  0.738120  0.755903  \n",
       "4          10     2.935539     3.406530  0.765453  0.663076  0.710596  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean BERTScore  P=0.7176  R=0.7119  F1=0.7142\n"
     ]
    }
   ],
   "source": [
    "logs = pd.read_csv(\"C:/Users/csode/project/evaluate/generated_responses_Llama-3.2-1B_FT_f32.gguf.csv\")\n",
    "evaluateBERTScore(CUSTOM_f32, logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4Imc0_ZKCpDG",
   "metadata": {
    "id": "4Imc0_ZKCpDG"
   },
   "source": [
    "## CUSTOM_f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70960c3a-2f91-4a19-9056-2266eb242baa",
   "metadata": {
    "id": "70960c3a-2f91-4a19-9056-2266eb242baa",
    "outputId": "387f668e-be20-467c-c1f9-8528bb684e6f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 147 tensors from /data/gguf/custom/Llama-3.2-1B_FT_f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Full_Model\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 1.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type  f16:  113 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 2.30 GiB (16.00 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 16\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.24 B\n",
      "print_info: general.name     = Full_Model\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: PAD token        = 128001 '<|end_of_text|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 162 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  2357.26 MiB\n",
      "..............................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   128.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   280.01 MiB\n",
      "llama_context: graph nodes  = 550\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '128256', 'general.file_type': '1', 'llama.attention.value_length': '64', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Full_Model', 'general.type': 'model', 'general.size_label': '1.2B', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '8192', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.block_count': '16', 'llama.attention.head_count_kv': '8', 'llama.attention.key_length': '64'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path   = f\"{MODEL_PATH}{CUSTOM_f16}\",\n",
    "    **params_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5780dadd-bc15-4313-8fc1-886a2bb3cda0",
   "metadata": {
    "id": "5780dadd-bc15-4313-8fc1-886a2bb3cda0",
    "outputId": "9a39b591-91f5-4b13-b880-d944317f2cbd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/100 [00:00<?, ?it/s]llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2494.53 ms /    28 tokens (   89.09 ms per token,    11.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1627.59 ms /    10 runs   (  162.76 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    4155.78 ms /    38 tokens\n",
      "Generating:   1%|          | 1/100 [00:04<06:52,  4.17s/it]Llama.generate: 18 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1088.13 ms /    13 tokens (   83.70 ms per token,    11.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9778.23 ms /    60 runs   (  162.97 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   11079.64 ms /    73 tokens\n",
      "Generating:   2%|▏         | 2/100 [00:15<13:27,  8.24s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1135.24 ms /    13 tokens (   87.33 ms per token,    11.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5889.69 ms /    36 runs   (  163.60 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    7150.77 ms /    49 tokens\n",
      "Generating:   3%|▎         | 3/100 [00:22<12:31,  7.75s/it]Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1358.89 ms /    16 tokens (   84.93 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7042.31 ms /    43 runs   (  163.77 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    8554.54 ms /    59 tokens\n",
      "Generating:   4%|▍         | 4/100 [00:30<12:55,  8.07s/it]Llama.generate: 19 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1665.79 ms /    20 tokens (   83.29 ms per token,    12.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1627.95 ms /    10 runs   (  162.79 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3327.97 ms /    30 tokens\n",
      "Generating:   5%|▌         | 5/100 [00:34<10:04,  6.37s/it]Llama.generate: 17 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1494.32 ms /    18 tokens (   83.02 ms per token,    12.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6046.17 ms /    37 runs   (  163.41 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    7672.63 ms /    55 tokens\n",
      "Generating:   6%|▌         | 6/100 [00:42<10:40,  6.81s/it]Llama.generate: 18 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1005.17 ms /    11 tokens (   91.38 ms per token,    10.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7837.90 ms /    48 runs   (  163.29 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    9013.00 ms /    59 tokens\n",
      "Generating:   7%|▋         | 7/100 [00:51<11:41,  7.54s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1165.88 ms /    14 tokens (   83.28 ms per token,    12.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4913.39 ms /    30 runs   (  163.78 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    6181.44 ms /    44 tokens\n",
      "Generating:   8%|▊         | 8/100 [00:57<10:53,  7.11s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1248.07 ms /    15 tokens (   83.20 ms per token,    12.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8501.16 ms /    52 runs   (  163.48 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    9929.41 ms /    67 tokens\n",
      "Generating:   9%|▉         | 9/100 [01:07<12:07,  7.99s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1047.38 ms /    12 tokens (   87.28 ms per token,    11.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10442.61 ms /    64 runs   (  163.17 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   11714.50 ms /    76 tokens\n",
      "Generating:  10%|█         | 10/100 [01:18<13:43,  9.15s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2136.94 ms /    24 tokens (   89.04 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10159.70 ms /    62 runs   (  163.87 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   12522.81 ms /    86 tokens\n",
      "Generating:  11%|█         | 11/100 [01:31<15:06, 10.19s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1621.46 ms /    19 tokens (   85.34 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8296.17 ms /    51 runs   (  162.67 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   10095.16 ms /    70 tokens\n",
      "Generating:  12%|█▏        | 12/100 [01:41<14:54, 10.16s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1797.75 ms /    22 tokens (   81.72 ms per token,    12.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2106.51 ms /    13 runs   (  162.04 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    3948.60 ms /    35 tokens\n",
      "Generating:  13%|█▎        | 13/100 [01:45<12:00,  8.28s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1326.57 ms /    16 tokens (   82.91 ms per token,    12.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10433.47 ms /    64 runs   (  163.02 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   11988.85 ms /    80 tokens\n",
      "Generating:  14%|█▍        | 14/100 [01:57<13:28,  9.41s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1156.08 ms /    14 tokens (   82.58 ms per token,    12.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11090.36 ms /    68 runs   (  163.09 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   12494.99 ms /    82 tokens\n",
      "Generating:  15%|█▌        | 15/100 [02:10<14:38, 10.34s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2195.84 ms /    26 tokens (   84.46 ms per token,    11.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10301.68 ms /    63 runs   (  163.52 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =   12720.52 ms /    89 tokens\n",
      "Generating:  16%|█▌        | 16/100 [02:22<15:29, 11.06s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1281.59 ms /    15 tokens (   85.44 ms per token,    11.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9174.91 ms /    56 runs   (  163.84 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   10653.96 ms /    71 tokens\n",
      "Generating:  17%|█▋        | 17/100 [02:33<15:08, 10.94s/it]Llama.generate: 17 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =     677.60 ms /     8 tokens (   84.70 ms per token,    11.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9456.42 ms /    58 runs   (  163.04 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   10346.95 ms /    66 tokens\n",
      "Generating:  18%|█▊        | 18/100 [02:43<14:42, 10.77s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1884.95 ms /    23 tokens (   81.95 ms per token,    12.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2618.88 ms /    16 runs   (  163.68 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    4557.59 ms /    39 tokens\n",
      "Generating:  19%|█▉        | 19/100 [02:48<12:01,  8.91s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1786.94 ms /    22 tokens (   81.22 ms per token,    12.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3113.94 ms /    19 runs   (  163.89 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    4965.50 ms /    41 tokens\n",
      "Generating:  20%|██        | 20/100 [02:53<10:18,  7.73s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1575.32 ms /    19 tokens (   82.91 ms per token,    12.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6370.18 ms /    39 runs   (  163.34 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    8078.43 ms /    58 tokens\n",
      "Generating:  21%|██        | 21/100 [03:01<10:19,  7.84s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1803.20 ms /    22 tokens (   81.96 ms per token,    12.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10941.38 ms /    67 runs   (  163.30 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =   12981.27 ms /    89 tokens\n",
      "Generating:  22%|██▏       | 22/100 [03:14<12:11,  9.38s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1158.26 ms /    14 tokens (   82.73 ms per token,    12.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5230.88 ms /    32 runs   (  163.47 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    6499.31 ms /    46 tokens\n",
      "Generating:  23%|██▎       | 23/100 [03:20<10:56,  8.52s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1428.27 ms /    16 tokens (   89.27 ms per token,    11.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6702.30 ms /    41 runs   (  163.47 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    8275.57 ms /    57 tokens\n",
      "Generating:  24%|██▍       | 24/100 [03:29<10:42,  8.45s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1916.41 ms /    23 tokens (   83.32 ms per token,    12.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2936.85 ms /    18 runs   (  163.16 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    4914.05 ms /    41 tokens\n",
      "Generating:  25%|██▌       | 25/100 [03:34<09:14,  7.39s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =     841.55 ms /    10 tokens (   84.16 ms per token,    11.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11135.97 ms /    68 runs   (  163.76 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   12220.89 ms /    78 tokens\n",
      "Generating:  26%|██▌       | 26/100 [03:46<10:54,  8.84s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1240.24 ms /    15 tokens (   82.68 ms per token,    12.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2755.83 ms /    17 runs   (  162.11 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    4053.56 ms /    32 tokens\n",
      "Generating:  27%|██▋       | 27/100 [03:50<09:00,  7.41s/it]Llama.generate: 18 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1744.66 ms /    21 tokens (   83.08 ms per token,    12.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12939.56 ms /    79 runs   (  163.79 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   14962.09 ms /   100 tokens\n",
      "Generating:  28%|██▊       | 28/100 [04:05<11:36,  9.68s/it]Llama.generate: 17 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =     767.53 ms /     9 tokens (   85.28 ms per token,    11.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7998.43 ms /    49 runs   (  163.23 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    8934.70 ms /    58 tokens\n",
      "Generating:  29%|██▉       | 29/100 [04:14<11:11,  9.46s/it]Llama.generate: 19 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =     930.52 ms /    11 tokens (   84.59 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8307.79 ms /    51 runs   (  162.90 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    9414.88 ms /    62 tokens\n",
      "Generating:  30%|███       | 30/100 [04:23<11:01,  9.45s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1230.55 ms /    15 tokens (   82.04 ms per token,    12.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6035.80 ms /    37 runs   (  163.13 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    7402.60 ms /    52 tokens\n",
      "Generating:  31%|███       | 31/100 [04:31<10:09,  8.84s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1643.64 ms /    20 tokens (   82.18 ms per token,    12.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7527.35 ms /    46 runs   (  163.64 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    9330.99 ms /    66 tokens\n",
      "Generating:  32%|███▏      | 32/100 [04:40<10:11,  8.99s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1009.97 ms /    12 tokens (   84.16 ms per token,    11.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4245.08 ms /    26 runs   (  163.27 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    5348.31 ms /    38 tokens\n",
      "Generating:  33%|███▎      | 33/100 [04:45<08:49,  7.90s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1003.60 ms /    12 tokens (   83.63 ms per token,    11.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4748.34 ms /    29 runs   (  163.74 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    5852.12 ms /    41 tokens\n",
      "Generating:  34%|███▍      | 34/100 [04:51<08:01,  7.29s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1397.84 ms /    17 tokens (   82.23 ms per token,    12.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2120.39 ms /    13 runs   (  163.11 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3562.37 ms /    30 tokens\n",
      "Generating:  35%|███▌      | 35/100 [04:55<06:41,  6.18s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1490.77 ms /    17 tokens (   87.69 ms per token,    11.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12599.75 ms /    77 runs   (  163.63 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   14358.02 ms /    94 tokens\n",
      "Generating:  36%|███▌      | 36/100 [05:09<09:12,  8.63s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1117.39 ms /    13 tokens (   85.95 ms per token,    11.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7192.41 ms /    44 runs   (  163.46 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    8469.19 ms /    57 tokens\n",
      "Generating:  37%|███▋      | 37/100 [05:18<09:01,  8.59s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1981.72 ms /    24 tokens (   82.57 ms per token,    12.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7837.52 ms /    48 runs   (  163.28 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    9986.83 ms /    72 tokens\n",
      "Generating:  38%|███▊      | 38/100 [05:28<09:18,  9.01s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1632.38 ms /    20 tokens (   81.62 ms per token,    12.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7986.07 ms /    49 runs   (  162.98 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    9787.41 ms /    69 tokens\n",
      "Generating:  39%|███▉      | 39/100 [05:37<09:24,  9.25s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1805.38 ms /    22 tokens (   82.06 ms per token,    12.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11552.02 ms /    71 runs   (  162.70 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   13609.81 ms /    93 tokens\n",
      "Generating:  40%|████      | 40/100 [05:51<10:33, 10.56s/it]Llama.generate: 19 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2203.34 ms /    26 tokens (   84.74 ms per token,    11.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7964.92 ms /    49 runs   (  162.55 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   10338.23 ms /    75 tokens\n",
      "Generating:  41%|████      | 41/100 [06:01<10:19, 10.50s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1566.77 ms /    19 tokens (   82.46 ms per token,    12.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5033.81 ms /    31 runs   (  162.38 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    6707.97 ms /    50 tokens\n",
      "Generating:  42%|████▏     | 42/100 [06:08<09:03,  9.36s/it]Llama.generate: 19 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =     727.62 ms /     8 tokens (   90.95 ms per token,    10.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3762.00 ms /    23 runs   (  163.57 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    4570.30 ms /    31 tokens\n",
      "Generating:  43%|████▎     | 43/100 [06:13<07:31,  7.93s/it]Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1780.03 ms /    22 tokens (   80.91 ms per token,    12.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10121.39 ms /    62 runs   (  163.25 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   12127.59 ms /    84 tokens\n",
      "Generating:  44%|████▍     | 44/100 [06:25<08:34,  9.19s/it]Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1397.41 ms /    17 tokens (   82.20 ms per token,    12.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3576.35 ms /    22 runs   (  162.56 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    5050.46 ms /    39 tokens\n",
      "Generating:  45%|████▌     | 45/100 [06:30<07:17,  7.95s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2045.83 ms /    25 tokens (   81.83 ms per token,    12.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6181.32 ms /    38 runs   (  162.67 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    8360.28 ms /    63 tokens\n",
      "Generating:  46%|████▌     | 46/100 [06:38<07:16,  8.08s/it]Llama.generate: 18 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1976.43 ms /    23 tokens (   85.93 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9485.90 ms /    58 runs   (  163.55 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   11666.81 ms /    81 tokens\n",
      "Generating:  47%|████▋     | 47/100 [06:50<08:05,  9.16s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1188.39 ms /    14 tokens (   84.89 ms per token,    11.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4706.94 ms /    29 runs   (  162.31 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    5994.59 ms /    43 tokens\n",
      "Generating:  48%|████▊     | 48/100 [06:56<07:07,  8.21s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1326.76 ms /    16 tokens (   82.92 ms per token,    12.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7963.54 ms /    49 runs   (  162.52 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    9457.96 ms /    65 tokens\n",
      "Generating:  49%|████▉     | 49/100 [07:05<07:18,  8.59s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1253.84 ms /    15 tokens (   83.59 ms per token,    11.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11672.16 ms /    72 runs   (  162.11 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   13181.44 ms /    87 tokens\n",
      "Generating:  50%|█████     | 50/100 [07:19<08:18,  9.97s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1617.43 ms /    19 tokens (   85.13 ms per token,    11.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2588.58 ms /    16 runs   (  161.79 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    4260.43 ms /    35 tokens\n",
      "Generating:  51%|█████     | 51/100 [07:23<06:44,  8.26s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2121.68 ms /    25 tokens (   84.87 ms per token,    11.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14131.99 ms /    87 runs   (  162.44 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   16592.55 ms /   112 tokens\n",
      "Generating:  52%|█████▏    | 52/100 [07:40<08:36, 10.77s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2111.01 ms /    24 tokens (   87.96 ms per token,    11.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2918.03 ms /    18 runs   (  162.11 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    5091.01 ms /    42 tokens\n",
      "Generating:  53%|█████▎    | 53/100 [07:45<07:06,  9.07s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1626.39 ms /    19 tokens (   85.60 ms per token,    11.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7103.78 ms /    44 runs   (  161.45 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    8893.21 ms /    63 tokens\n",
      "Generating:  54%|█████▍    | 54/100 [07:54<06:54,  9.02s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1367.85 ms /    16 tokens (   85.49 ms per token,    11.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5162.43 ms /    32 runs   (  161.33 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    6639.68 ms /    48 tokens\n",
      "Generating:  55%|█████▌    | 55/100 [08:00<06:13,  8.31s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1235.55 ms /    14 tokens (   88.25 ms per token,    11.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8888.32 ms /    55 runs   (  161.61 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   10332.64 ms /    69 tokens\n",
      "Generating:  56%|█████▌    | 56/100 [08:11<06:32,  8.92s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1958.69 ms /    22 tokens (   89.03 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8898.03 ms /    55 runs   (  161.78 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   11048.21 ms /    77 tokens\n",
      "Generating:  57%|█████▋    | 57/100 [08:22<06:51,  9.56s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1822.74 ms /    22 tokens (   82.85 ms per token,    12.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7628.89 ms /    47 runs   (  162.32 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    9612.97 ms /    69 tokens\n",
      "Generating:  58%|█████▊    | 58/100 [08:31<06:42,  9.58s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1264.46 ms /    15 tokens (   84.30 ms per token,    11.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14106.54 ms /    87 runs   (  162.14 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   15693.07 ms /   102 tokens\n",
      "Generating:  59%|█████▉    | 59/100 [08:47<07:48, 11.42s/it]Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1493.53 ms /    17 tokens (   87.85 ms per token,    11.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9586.83 ms /    59 runs   (  162.49 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   11286.55 ms /    76 tokens\n",
      "Generating:  60%|██████    | 60/100 [08:58<07:35, 11.39s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1153.61 ms /    14 tokens (   82.40 ms per token,    12.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12032.57 ms /    74 runs   (  162.60 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   13443.28 ms /    88 tokens\n",
      "Generating:  61%|██████    | 61/100 [09:12<07:48, 12.01s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1817.88 ms /    22 tokens (   82.63 ms per token,    12.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5192.43 ms /    32 runs   (  162.26 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    7120.33 ms /    54 tokens\n",
      "Generating:  62%|██████▏   | 62/100 [09:19<06:40, 10.55s/it]Llama.generate: 19 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1997.17 ms /    24 tokens (   83.22 ms per token,    12.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5357.76 ms /    33 runs   (  162.36 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    7467.40 ms /    57 tokens\n",
      "Generating:  63%|██████▎   | 63/100 [09:26<05:56,  9.63s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1176.85 ms /    14 tokens (   84.06 ms per token,    11.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4690.60 ms /    29 runs   (  161.74 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    5966.65 ms /    43 tokens\n",
      "Generating:  64%|██████▍   | 64/100 [09:32<05:07,  8.53s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1623.72 ms /    19 tokens (   85.46 ms per token,    11.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12284.00 ms /    75 runs   (  163.79 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   14174.67 ms /    94 tokens\n",
      "Generating:  65%|██████▌   | 65/100 [09:47<05:57, 10.23s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1711.77 ms /    20 tokens (   85.59 ms per token,    11.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15038.02 ms /    92 runs   (  163.46 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =   17083.17 ms /   112 tokens\n",
      "Generating:  66%|██████▌   | 66/100 [10:04<06:57, 12.29s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1614.37 ms /    19 tokens (   84.97 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1795.74 ms /    11 runs   (  163.25 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3448.29 ms /    30 tokens\n",
      "Generating:  67%|██████▋   | 67/100 [10:07<05:18,  9.64s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1279.63 ms /    15 tokens (   85.31 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8789.24 ms /    54 runs   (  162.76 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   10254.78 ms /    69 tokens\n",
      "Generating:  68%|██████▊   | 68/100 [10:17<05:14,  9.83s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1177.54 ms /    14 tokens (   84.11 ms per token,    11.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7589.58 ms /    47 runs   (  161.48 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    8928.99 ms /    61 tokens\n",
      "Generating:  69%|██████▉   | 69/100 [10:26<04:56,  9.56s/it]Llama.generate: 19 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1042.68 ms /    12 tokens (   86.89 ms per token,    11.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9227.19 ms /    57 runs   (  161.88 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   10466.97 ms /    69 tokens\n",
      "Generating:  70%|███████   | 70/100 [10:37<04:55,  9.84s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1600.14 ms /    19 tokens (   84.22 ms per token,    11.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6168.52 ms /    38 runs   (  162.33 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    7898.29 ms /    57 tokens\n",
      "Generating:  71%|███████   | 71/100 [10:45<04:28,  9.26s/it]Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1883.04 ms /    22 tokens (   85.59 ms per token,    11.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2594.44 ms /    16 runs   (  162.15 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    4530.12 ms /    38 tokens\n",
      "Generating:  72%|███████▏  | 72/100 [10:49<03:39,  7.84s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2028.90 ms /    24 tokens (   84.54 ms per token,    11.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5524.94 ms /    34 runs   (  162.50 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    7671.13 ms /    58 tokens\n",
      "Generating:  73%|███████▎  | 73/100 [10:57<03:30,  7.79s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2133.36 ms /    25 tokens (   85.33 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5538.62 ms /    34 runs   (  162.90 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    7789.86 ms /    59 tokens\n",
      "Generating:  74%|███████▍  | 74/100 [11:05<03:22,  7.80s/it]Llama.generate: 19 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1918.51 ms /    23 tokens (   83.41 ms per token,    11.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9108.76 ms /    56 runs   (  162.66 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   11221.85 ms /    79 tokens\n",
      "Generating:  75%|███████▌  | 75/100 [11:16<03:40,  8.83s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1637.36 ms /    19 tokens (   86.18 ms per token,    11.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18887.88 ms /   116 runs   (  162.83 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   20929.37 ms /   135 tokens\n",
      "Generating:  76%|███████▌  | 76/100 [11:37<04:59, 12.46s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1332.71 ms /    16 tokens (   83.29 ms per token,    12.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9104.90 ms /    56 runs   (  162.59 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   10634.09 ms /    72 tokens\n",
      "Generating:  77%|███████▋  | 77/100 [11:48<04:34, 11.92s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2129.66 ms /    26 tokens (   81.91 ms per token,    12.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9922.90 ms /    61 runs   (  162.67 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   12267.63 ms /    87 tokens\n",
      "Generating:  78%|███████▊  | 78/100 [12:00<04:24, 12.03s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1589.18 ms /    19 tokens (   83.64 ms per token,    11.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12670.73 ms /    78 runs   (  162.45 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   14538.96 ms /    97 tokens\n",
      "Generating:  79%|███████▉  | 79/100 [12:14<04:28, 12.78s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2037.69 ms /    25 tokens (   81.51 ms per token,    12.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8937.57 ms /    55 runs   (  162.50 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   11172.66 ms /    80 tokens\n",
      "Generating:  80%|████████  | 80/100 [12:26<04:06, 12.31s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1734.90 ms /    20 tokens (   86.74 ms per token,    11.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5058.97 ms /    31 runs   (  163.19 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    6902.34 ms /    51 tokens\n",
      "Generating:  81%|████████  | 81/100 [12:32<03:23, 10.69s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1980.42 ms /    24 tokens (   82.52 ms per token,    12.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6353.13 ms /    39 runs   (  162.90 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    8469.83 ms /    63 tokens\n",
      "Generating:  82%|████████▏ | 82/100 [12:41<03:00, 10.03s/it]Llama.generate: 17 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =     929.02 ms /    11 tokens (   84.46 ms per token,    11.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7333.38 ms /    45 runs   (  162.96 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    8418.14 ms /    56 tokens\n",
      "Generating:  83%|████████▎ | 83/100 [12:49<02:42,  9.55s/it]Llama.generate: 17 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1711.23 ms /    21 tokens (   81.49 ms per token,    12.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25379.75 ms /   156 runs   (  162.69 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   27640.12 ms /   177 tokens\n",
      "Generating:  84%|████████▍ | 84/100 [13:17<03:59, 14.98s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1339.85 ms /    16 tokens (   83.74 ms per token,    11.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8078.66 ms /    50 runs   (  161.57 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    9600.29 ms /    66 tokens\n",
      "Generating:  85%|████████▌ | 85/100 [13:27<03:20, 13.37s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1252.12 ms /    15 tokens (   83.47 ms per token,    11.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2279.87 ms /    14 runs   (  162.85 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3580.61 ms /    29 tokens\n",
      "Generating:  86%|████████▌ | 86/100 [13:30<02:26, 10.43s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1640.93 ms /    20 tokens (   82.05 ms per token,    12.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2101.99 ms /    13 runs   (  161.69 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    3789.19 ms /    33 tokens\n",
      "Generating:  87%|████████▋ | 87/100 [13:34<01:49,  8.44s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1812.52 ms /    22 tokens (   82.39 ms per token,    12.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8288.85 ms /    51 runs   (  162.53 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   10280.80 ms /    73 tokens\n",
      "Generating:  88%|████████▊ | 88/100 [13:44<01:47,  9.00s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1432.91 ms /    16 tokens (   89.56 ms per token,    11.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4665.47 ms /    29 runs   (  160.88 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6197.26 ms /    45 tokens\n",
      "Generating:  89%|████████▉ | 89/100 [13:51<01:29,  8.16s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1885.62 ms /    23 tokens (   81.98 ms per token,    12.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7814.97 ms /    48 runs   (  162.81 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    9865.82 ms /    71 tokens\n",
      "Generating:  90%|█████████ | 90/100 [14:00<01:26,  8.68s/it]Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1397.38 ms /    16 tokens (   87.34 ms per token,    11.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13799.58 ms /    85 runs   (  162.35 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   15494.49 ms /   101 tokens\n",
      "Generating:  91%|█████████ | 91/100 [14:16<01:36, 10.73s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1630.55 ms /    20 tokens (   81.53 ms per token,    12.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5021.05 ms /    31 runs   (  161.97 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    6757.32 ms /    51 tokens\n",
      "Generating:  92%|█████████▏| 92/100 [14:23<01:16,  9.54s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1403.15 ms /    17 tokens (   82.54 ms per token,    12.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6644.81 ms /    41 runs   (  162.07 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    8189.81 ms /    58 tokens\n",
      "Generating:  93%|█████████▎| 93/100 [14:31<01:03,  9.14s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1683.56 ms /    20 tokens (   84.18 ms per token,    11.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6482.28 ms /    40 runs   (  162.06 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    8305.08 ms /    60 tokens\n",
      "Generating:  94%|█████████▍| 94/100 [14:39<00:53,  8.89s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1926.45 ms /    23 tokens (   83.76 ms per token,    11.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6489.90 ms /    40 runs   (  162.25 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    8553.27 ms /    63 tokens\n",
      "Generating:  95%|█████████▌| 95/100 [14:48<00:43,  8.79s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2111.13 ms /    26 tokens (   81.20 ms per token,    12.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10244.04 ms /    63 runs   (  162.60 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   12575.01 ms /    89 tokens\n",
      "Generating:  96%|█████████▌| 96/100 [15:00<00:39,  9.93s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1018.98 ms /    12 tokens (   84.92 ms per token,    11.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7775.03 ms /    48 runs   (  161.98 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    8957.03 ms /    60 tokens\n",
      "Generating:  97%|█████████▋| 97/100 [15:09<00:28,  9.64s/it]Llama.generate: 19 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2147.69 ms /    25 tokens (   85.91 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7138.06 ms /    44 runs   (  162.23 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    9437.78 ms /    69 tokens\n",
      "Generating:  98%|█████████▊| 98/100 [15:19<00:19,  9.58s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1967.46 ms /    24 tokens (   81.98 ms per token,    12.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21327.24 ms /   131 runs   (  162.80 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   23773.98 ms /   155 tokens\n",
      "Generating:  99%|█████████▉| 99/100 [15:43<00:13, 13.84s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2494.92 ms\n",
      "llama_perf_context_print: prompt eval time =     885.69 ms /    10 tokens (   88.57 ms per token,    11.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10990.16 ms /    68 runs   (  161.62 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   12113.69 ms /    78 tokens\n",
      "Generating: 100%|██████████| 100/100 [15:55<00:00,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for model  Llama-3.2-1B_FT_f16.gguf\n",
      "Total 100 examples,  955.2s\n",
      "Mean latency      9.549s\n",
      "Mean throughput   5.11 tok/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logs = genDataset(llm, evaluate_df, CUSTOM_f16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e85af1fb-a10d-4228-9165-ebc8574e20a9",
   "metadata": {
    "id": "e85af1fb-a10d-4228-9165-ebc8574e20a9",
    "outputId": "fd874c66-31a6-413d-cb34-53cdd971c72e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8.34 seconds, 11.99 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Baseline not Found for bert-base-multilingual-cased on ru at C:\\Tools\\anaconda3\\envs\\llm_in_finance\\lib\\site-packages\\bert_score\\rescale_baseline/ru/bert-base-multilingual-cased.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>pred</th>\n",
       "      <th>ref</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>gen_tokens</th>\n",
       "      <th>latency_sec</th>\n",
       "      <th>tok_per_sec</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Тот, кто уплачивает налог.</td>\n",
       "      <td>Налогоплательщик — это физическое или юридичес...</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>4.164381</td>\n",
       "      <td>2.641449</td>\n",
       "      <td>0.731216</td>\n",
       "      <td>0.621312</td>\n",
       "      <td>0.671799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Далее следует указание размера обязательного н...</td>\n",
       "      <td>Налоговое обязательство включает в себя обязан...</td>\n",
       "      <td>31</td>\n",
       "      <td>61</td>\n",
       "      <td>11.090358</td>\n",
       "      <td>5.500273</td>\n",
       "      <td>0.661250</td>\n",
       "      <td>0.683022</td>\n",
       "      <td>0.671960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Налогоплательщик имеет право обжаловать налого...</td>\n",
       "      <td>Налогоплательщик имеет право получать разъясне...</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>7.159261</td>\n",
       "      <td>5.168131</td>\n",
       "      <td>0.798107</td>\n",
       "      <td>0.749012</td>\n",
       "      <td>0.772781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Налогоплательщик обязан предоставить налоговом...</td>\n",
       "      <td>Налогоплательщик обязан своевременно и в полно...</td>\n",
       "      <td>35</td>\n",
       "      <td>44</td>\n",
       "      <td>8.568267</td>\n",
       "      <td>5.135227</td>\n",
       "      <td>0.774563</td>\n",
       "      <td>0.738120</td>\n",
       "      <td>0.755903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Налог на прибыль, НДС.</td>\n",
       "      <td>Налоговым кодексом установлены налоги на доход...</td>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>3.337609</td>\n",
       "      <td>2.996157</td>\n",
       "      <td>0.765453</td>\n",
       "      <td>0.663076</td>\n",
       "      <td>0.710596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                               pred  \\\n",
       "0    0                         Тот, кто уплачивает налог.   \n",
       "1    1  Далее следует указание размера обязательного н...   \n",
       "2    2  Налогоплательщик имеет право обжаловать налого...   \n",
       "3    3  Налогоплательщик обязан предоставить налоговом...   \n",
       "4    4                             Налог на прибыль, НДС.   \n",
       "\n",
       "                                                 ref  prompt_tokens  \\\n",
       "0  Налогоплательщик — это физическое или юридичес...             28   \n",
       "1  Налоговое обязательство включает в себя обязан...             31   \n",
       "2  Налогоплательщик имеет право получать разъясне...             30   \n",
       "3  Налогоплательщик обязан своевременно и в полно...             35   \n",
       "4  Налоговым кодексом установлены налоги на доход...             39   \n",
       "\n",
       "   gen_tokens  latency_sec  tok_per_sec         P         R        F1  \n",
       "0          11     4.164381     2.641449  0.731216  0.621312  0.671799  \n",
       "1          61    11.090358     5.500273  0.661250  0.683022  0.671960  \n",
       "2          37     7.159261     5.168131  0.798107  0.749012  0.772781  \n",
       "3          44     8.568267     5.135227  0.774563  0.738120  0.755903  \n",
       "4          10     3.337609     2.996157  0.765453  0.663076  0.710596  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean BERTScore  P=0.7181  R=0.7151  F1=0.7160\n"
     ]
    }
   ],
   "source": [
    "logs = pd.read_csv(\"C:/Users/csode/project/evaluate/generated_responses_Llama-3.2-1B_FT_f16.gguf.csv\")\n",
    "evaluateBERTScore(CUSTOM_f16, logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XUGsCWDmC0V-",
   "metadata": {
    "id": "XUGsCWDmC0V-"
   },
   "source": [
    "## CUSTOM_q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fBl2PO3EBj2",
   "metadata": {
    "id": "9fBl2PO3EBj2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 147 tensors from /data/gguf/custom/Llama-3.2-1B_FT_q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Full_Model\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 1.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q8_0:  113 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 1.22 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 16\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.24 B\n",
      "print_info: general.name     = Full_Model\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: PAD token        = 128001 '<|end_of_text|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 162 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB\n",
      "..............................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   128.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   280.01 MiB\n",
      "llama_context: graph nodes  = 550\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.attention.value_length': '64', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Full_Model', 'general.type': 'model', 'general.size_label': '1.2B', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '8192', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.block_count': '16', 'llama.attention.head_count_kv': '8', 'llama.attention.key_length': '64'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path   = f\"{MODEL_PATH}{CUSTOM_q8_0}\",\n",
    "    **params_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "027nxCIvECVM",
   "metadata": {
    "id": "027nxCIvECVM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/100 [00:00<?, ?it/s]llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     374.63 ms /    28 tokens (   13.38 ms per token,    74.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =     795.74 ms /    10 runs   (   79.57 ms per token,    12.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.46 ms /    38 tokens\n",
      "Generating:   1%|          | 1/100 [00:01<02:00,  1.22s/it]Llama.generate: 18 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     148.63 ms /    13 tokens (   11.43 ms per token,    87.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6709.81 ms /    83 runs   (   80.84 ms per token,    12.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    7221.26 ms /    96 tokens\n",
      "Generating:   2%|▏         | 2/100 [00:08<07:46,  4.76s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     148.33 ms /    13 tokens (   11.41 ms per token,    87.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4567.79 ms /    57 runs   (   80.14 ms per token,    12.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    4959.51 ms /    70 tokens\n",
      "Generating:   3%|▎         | 3/100 [00:13<07:51,  4.86s/it]Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     216.45 ms /    16 tokens (   13.53 ms per token,    73.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4054.08 ms /    50 runs   (   81.08 ms per token,    12.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4495.40 ms /    66 tokens\n",
      "Generating:   4%|▍         | 4/100 [00:17<07:33,  4.72s/it]Llama.generate: 19 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     396.18 ms /    20 tokens (   19.81 ms per token,    50.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2658.63 ms /    33 runs   (   80.56 ms per token,    12.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    3193.74 ms /    53 tokens\n",
      "Generating:   5%|▌         | 5/100 [00:21<06:36,  4.18s/it]Llama.generate: 17 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     215.84 ms /    18 tokens (   11.99 ms per token,    83.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4957.69 ms /    62 runs   (   79.96 ms per token,    12.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    5450.26 ms /    80 tokens\n",
      "Generating:   6%|▌         | 6/100 [00:26<07:13,  4.61s/it]Llama.generate: 18 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     133.61 ms /    11 tokens (   12.15 ms per token,    82.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1658.16 ms /    21 runs   (   78.96 ms per token,    12.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1878.21 ms /    32 tokens\n",
      "Generating:   7%|▋         | 7/100 [00:28<05:46,  3.72s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     198.84 ms /    14 tokens (   14.20 ms per token,    70.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1957.81 ms /    25 runs   (   78.31 ms per token,    12.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    2271.31 ms /    39 tokens\n",
      "Generating:   8%|▊         | 8/100 [00:30<05:00,  3.27s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     229.59 ms /    15 tokens (   15.31 ms per token,    65.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3235.79 ms /    41 runs   (   78.92 ms per token,    12.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    3642.55 ms /    56 tokens\n",
      "Generating:   9%|▉         | 9/100 [00:34<05:08,  3.39s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     145.95 ms /    12 tokens (   12.16 ms per token,    82.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4029.26 ms /    51 runs   (   79.01 ms per token,    12.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    4398.59 ms /    63 tokens\n",
      "Generating:  10%|█         | 10/100 [00:38<05:33,  3.70s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     256.64 ms /    24 tokens (   10.69 ms per token,    93.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3170.91 ms /    40 runs   (   79.27 ms per token,    12.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    3601.81 ms /    64 tokens\n",
      "Generating:  11%|█         | 11/100 [00:42<05:27,  3.68s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     293.36 ms /    19 tokens (   15.44 ms per token,    64.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7382.82 ms /    93 runs   (   79.39 ms per token,    12.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    8074.51 ms /   112 tokens\n",
      "Generating:  12%|█▏        | 12/100 [00:50<07:21,  5.02s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     242.19 ms /    22 tokens (   11.01 ms per token,    90.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1044.51 ms /    13 runs   (   80.35 ms per token,    12.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1339.68 ms /    35 tokens\n",
      "Generating:  13%|█▎        | 13/100 [00:51<05:40,  3.91s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     215.77 ms /    16 tokens (   13.49 ms per token,    74.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6071.65 ms /    76 runs   (   79.89 ms per token,    12.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    6618.85 ms /    92 tokens\n",
      "Generating:  14%|█▍        | 14/100 [00:58<06:47,  4.73s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     159.41 ms /    14 tokens (   11.39 ms per token,    87.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7884.22 ms /    99 runs   (   79.64 ms per token,    12.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    8469.46 ms /   113 tokens\n",
      "Generating:  15%|█▌        | 15/100 [01:07<08:18,  5.86s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     319.63 ms /    26 tokens (   12.29 ms per token,    81.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4694.57 ms /    59 runs   (   79.57 ms per token,    12.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    5260.55 ms /    85 tokens\n",
      "Generating:  16%|█▌        | 16/100 [01:12<07:57,  5.69s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     165.33 ms /    15 tokens (   11.02 ms per token,    90.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2902.99 ms /    37 runs   (   78.46 ms per token,    12.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3220.87 ms /    52 tokens\n",
      "Generating:  17%|█▋        | 17/100 [01:15<06:50,  4.95s/it]Llama.generate: 17 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     122.91 ms /     8 tokens (   15.36 ms per token,    65.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6433.12 ms /    81 runs   (   79.42 ms per token,    12.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    6913.04 ms /    89 tokens\n",
      "Generating:  18%|█▊        | 18/100 [01:22<07:34,  5.54s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     256.48 ms /    23 tokens (   11.15 ms per token,    89.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1338.42 ms /    17 runs   (   78.73 ms per token,    12.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1662.79 ms /    40 tokens\n",
      "Generating:  19%|█▉        | 19/100 [01:24<05:54,  4.38s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     243.93 ms /    22 tokens (   11.09 ms per token,    90.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2622.68 ms /    33 runs   (   79.48 ms per token,    12.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    3013.47 ms /    55 tokens\n",
      "Generating:  20%|██        | 20/100 [01:27<05:17,  3.97s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     268.54 ms /    19 tokens (   14.13 ms per token,    70.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2602.99 ms /    33 runs   (   78.88 ms per token,    12.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    3019.18 ms /    52 tokens\n",
      "Generating:  21%|██        | 21/100 [01:30<04:51,  3.69s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     239.69 ms /    22 tokens (   10.89 ms per token,    91.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4678.13 ms /    59 runs   (   79.29 ms per token,    12.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    5169.01 ms /    81 tokens\n",
      "Generating:  22%|██▏       | 22/100 [01:35<05:22,  4.14s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     177.65 ms /    14 tokens (   12.69 ms per token,    78.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2608.80 ms /    33 runs   (   79.05 ms per token,    12.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    2928.16 ms /    47 tokens\n",
      "Generating:  23%|██▎       | 23/100 [01:38<04:51,  3.78s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     186.99 ms /    16 tokens (   11.69 ms per token,    85.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3448.63 ms /    43 runs   (   80.20 ms per token,    12.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    3813.48 ms /    59 tokens\n",
      "Generating:  24%|██▍       | 24/100 [01:42<04:48,  3.80s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     270.30 ms /    23 tokens (   11.75 ms per token,    85.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1422.22 ms /    18 runs   (   79.01 ms per token,    12.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1765.04 ms /    41 tokens\n",
      "Generating:  25%|██▌       | 25/100 [01:43<03:59,  3.19s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     119.26 ms /    10 tokens (   11.93 ms per token,    83.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4229.76 ms /    54 runs   (   78.33 ms per token,    12.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    4577.14 ms /    64 tokens\n",
      "Generating:  26%|██▌       | 26/100 [01:48<04:27,  3.61s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     179.37 ms /    15 tokens (   11.96 ms per token,    83.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1343.03 ms /    17 runs   (   79.00 ms per token,    12.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1594.57 ms /    32 tokens\n",
      "Generating:  27%|██▋       | 27/100 [01:50<03:39,  3.01s/it]Llama.generate: 18 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     228.55 ms /    21 tokens (   10.88 ms per token,    91.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3068.02 ms /    38 runs   (   80.74 ms per token,    12.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    3453.78 ms /    59 tokens\n",
      "Generating:  28%|██▊       | 28/100 [01:53<03:46,  3.15s/it]Llama.generate: 17 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     120.87 ms /     9 tokens (   13.43 ms per token,    74.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4617.02 ms /    58 runs   (   79.60 ms per token,    12.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    4982.30 ms /    67 tokens\n",
      "Generating:  29%|██▉       | 29/100 [01:58<04:23,  3.71s/it]Llama.generate: 19 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     165.48 ms /    11 tokens (   15.04 ms per token,    66.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6625.03 ms /    83 runs   (   79.82 ms per token,    12.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    7175.17 ms /    94 tokens\n",
      "Generating:  30%|███       | 30/100 [02:05<05:32,  4.75s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     190.61 ms /    15 tokens (   12.71 ms per token,    78.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3615.42 ms /    46 runs   (   78.60 ms per token,    12.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    3999.84 ms /    61 tokens\n",
      "Generating:  31%|███       | 31/100 [02:09<05:12,  4.53s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     226.07 ms /    20 tokens (   11.30 ms per token,    88.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3054.53 ms /    39 runs   (   78.32 ms per token,    12.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    3445.15 ms /    59 tokens\n",
      "Generating:  32%|███▏      | 32/100 [02:13<04:46,  4.21s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     151.12 ms /    12 tokens (   12.59 ms per token,    79.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5109.80 ms /    64 runs   (   79.84 ms per token,    12.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    5539.07 ms /    76 tokens\n",
      "Generating:  33%|███▎      | 33/100 [02:18<05:09,  4.62s/it]Llama.generate: 17 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     182.05 ms /    12 tokens (   15.17 ms per token,    65.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4049.34 ms /    51 runs   (   79.40 ms per token,    12.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    4442.09 ms /    63 tokens\n",
      "Generating:  34%|███▍      | 34/100 [02:23<05:01,  4.57s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     183.24 ms /    17 tokens (   10.78 ms per token,    92.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1262.12 ms /    16 runs   (   78.88 ms per token,    12.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1509.23 ms /    33 tokens\n",
      "Generating:  35%|███▌      | 35/100 [02:24<03:57,  3.66s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     183.20 ms /    17 tokens (   10.78 ms per token,    92.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3557.61 ms /    44 runs   (   80.85 ms per token,    12.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    3922.37 ms /    61 tokens\n",
      "Generating:  36%|███▌      | 36/100 [02:28<03:59,  3.74s/it]Llama.generate: 17 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     203.19 ms /    13 tokens (   15.63 ms per token,    63.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2874.17 ms /    35 runs   (   82.12 ms per token,    12.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    3221.64 ms /    48 tokens\n",
      "Generating:  37%|███▋      | 37/100 [02:32<03:46,  3.59s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     270.14 ms /    24 tokens (   11.26 ms per token,    88.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4642.58 ms /    58 runs   (   80.04 ms per token,    12.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    5155.25 ms /    82 tokens\n",
      "Generating:  38%|███▊      | 38/100 [02:37<04:12,  4.07s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     228.02 ms /    20 tokens (   11.40 ms per token,    87.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3092.20 ms /    39 runs   (   79.29 ms per token,    12.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    3482.09 ms /    59 tokens\n",
      "Generating:  39%|███▉      | 39/100 [02:40<03:57,  3.90s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     234.11 ms /    22 tokens (   10.64 ms per token,    93.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2232.63 ms /    28 runs   (   79.74 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    2580.64 ms /    50 tokens\n",
      "Generating:  40%|████      | 40/100 [02:43<03:30,  3.51s/it]Llama.generate: 19 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     310.42 ms /    26 tokens (   11.94 ms per token,    83.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1524.86 ms /    19 runs   (   80.26 ms per token,    12.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1911.22 ms /    45 tokens\n",
      "Generating:  41%|████      | 41/100 [02:45<02:58,  3.03s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     231.56 ms /    19 tokens (   12.19 ms per token,    82.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4283.12 ms /    53 runs   (   80.81 ms per token,    12.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    4735.45 ms /    72 tokens\n",
      "Generating:  42%|████▏     | 42/100 [02:49<03:25,  3.55s/it]Llama.generate: 19 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     106.11 ms /     8 tokens (   13.26 ms per token,    75.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2683.46 ms /    34 runs   (   78.93 ms per token,    12.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    2931.52 ms /    42 tokens\n",
      "Generating:  43%|████▎     | 43/100 [02:52<03:11,  3.37s/it]Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     234.22 ms /    22 tokens (   10.65 ms per token,    93.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3168.99 ms /    40 runs   (   79.22 ms per token,    12.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    3570.55 ms /    62 tokens\n",
      "Generating:  44%|████▍     | 44/100 [02:56<03:12,  3.43s/it]Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     212.66 ms /    17 tokens (   12.51 ms per token,    79.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1021.49 ms /    13 runs   (   78.58 ms per token,    12.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.09 ms /    30 tokens\n",
      "Generating:  45%|████▌     | 45/100 [02:57<02:33,  2.79s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     259.74 ms /    25 tokens (   10.39 ms per token,    96.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5022.67 ms /    63 runs   (   79.72 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    5549.51 ms /    88 tokens\n",
      "Generating:  46%|████▌     | 46/100 [03:03<03:15,  3.62s/it]Llama.generate: 18 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     241.46 ms /    23 tokens (   10.50 ms per token,    95.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6025.74 ms /    76 runs   (   79.29 ms per token,    12.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    6596.35 ms /    99 tokens\n",
      "Generating:  47%|████▋     | 47/100 [03:09<03:59,  4.52s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     162.42 ms /    14 tokens (   11.60 ms per token,    86.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2772.51 ms /    35 runs   (   79.21 ms per token,    12.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    3079.28 ms /    49 tokens\n",
      "Generating:  48%|████▊     | 48/100 [03:13<03:32,  4.09s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     183.77 ms /    16 tokens (   11.49 ms per token,    87.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6786.37 ms /    86 runs   (   78.91 ms per token,    12.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    7349.46 ms /   102 tokens\n",
      "Generating:  49%|████▉     | 49/100 [03:20<04:18,  5.07s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     174.19 ms /    15 tokens (   11.61 ms per token,    86.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6784.50 ms /    85 runs   (   79.82 ms per token,    12.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    7348.26 ms /   100 tokens\n",
      "Generating:  50%|█████     | 50/100 [03:27<04:47,  5.76s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     217.11 ms /    19 tokens (   11.43 ms per token,    87.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1022.60 ms /    13 runs   (   78.66 ms per token,    12.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1291.49 ms /    32 tokens\n",
      "Generating:  51%|█████     | 51/100 [03:29<03:36,  4.42s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     260.21 ms /    25 tokens (   10.41 ms per token,    96.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12537.43 ms /   156 runs   (   80.37 ms per token,    12.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   13454.09 ms /   181 tokens\n",
      "Generating:  52%|█████▏    | 52/100 [03:42<05:42,  7.13s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     296.52 ms /    24 tokens (   12.36 ms per token,    80.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3668.97 ms /    46 runs   (   79.76 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    4153.90 ms /    70 tokens\n",
      "Generating:  53%|█████▎    | 53/100 [03:46<04:53,  6.24s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     216.52 ms /    19 tokens (   11.40 ms per token,    87.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6942.16 ms /    87 runs   (   79.80 ms per token,    12.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    7538.73 ms /   106 tokens\n",
      "Generating:  54%|█████▍    | 54/100 [03:54<05:05,  6.64s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     271.30 ms /    16 tokens (   16.96 ms per token,    58.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1735.60 ms /    22 runs   (   78.89 ms per token,    12.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    2096.85 ms /    38 tokens\n",
      "Generating:  55%|█████▌    | 55/100 [03:56<03:57,  5.28s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     164.34 ms /    14 tokens (   11.74 ms per token,    85.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3920.70 ms /    50 runs   (   78.41 ms per token,    12.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    4292.68 ms /    64 tokens\n",
      "Generating:  56%|█████▌    | 56/100 [04:00<03:39,  4.99s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     248.80 ms /    22 tokens (   11.31 ms per token,    88.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8900.11 ms /   112 runs   (   79.47 ms per token,    12.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    9611.83 ms /   134 tokens\n",
      "Generating:  57%|█████▋    | 57/100 [04:10<04:34,  6.38s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     243.41 ms /    22 tokens (   11.06 ms per token,    90.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1902.80 ms /    24 runs   (   79.28 ms per token,    12.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    2243.42 ms /    46 tokens\n",
      "Generating:  58%|█████▊    | 58/100 [04:12<03:36,  5.14s/it]Llama.generate: 18 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     190.99 ms /    15 tokens (   12.73 ms per token,    78.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4010.22 ms /    51 runs   (   78.63 ms per token,    12.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    4411.56 ms /    66 tokens\n",
      "Generating:  59%|█████▉    | 59/100 [04:17<03:22,  4.93s/it]Llama.generate: 18 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     230.96 ms /    17 tokens (   13.59 ms per token,    73.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5034.81 ms /    64 runs   (   78.67 ms per token,    12.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    5533.20 ms /    81 tokens\n",
      "Generating:  60%|██████    | 60/100 [04:22<03:24,  5.11s/it]Llama.generate: 18 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     155.21 ms /    14 tokens (   11.09 ms per token,    90.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1913.16 ms /    24 runs   (   79.72 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    2164.51 ms /    38 tokens\n",
      "Generating:  61%|██████    | 61/100 [04:24<02:45,  4.23s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     255.97 ms /    22 tokens (   11.63 ms per token,    85.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5811.19 ms /    74 runs   (   78.53 ms per token,    12.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    6384.45 ms /    96 tokens\n",
      "Generating:  62%|██████▏   | 62/100 [04:31<03:05,  4.88s/it]Llama.generate: 19 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     265.19 ms /    24 tokens (   11.05 ms per token,    90.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2146.21 ms /    27 runs   (   79.49 ms per token,    12.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    2520.96 ms /    51 tokens\n",
      "Generating:  63%|██████▎   | 63/100 [04:33<02:34,  4.18s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     165.17 ms /    14 tokens (   11.80 ms per token,    84.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2414.06 ms /    31 runs   (   77.87 ms per token,    12.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    2707.05 ms /    45 tokens\n",
      "Generating:  64%|██████▍   | 64/100 [04:36<02:14,  3.74s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     259.84 ms /    19 tokens (   13.68 ms per token,    73.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2655.09 ms /    34 runs   (   78.09 ms per token,    12.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    3054.44 ms /    53 tokens\n",
      "Generating:  65%|██████▌   | 65/100 [04:39<02:03,  3.54s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     293.63 ms /    20 tokens (   14.68 ms per token,    68.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2593.61 ms /    33 runs   (   78.59 ms per token,    12.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    3022.38 ms /    53 tokens\n",
      "Generating:  66%|██████▌   | 66/100 [04:42<01:55,  3.39s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     215.76 ms /    19 tokens (   11.36 ms per token,    88.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3416.46 ms /    43 runs   (   79.45 ms per token,    12.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    3827.63 ms /    62 tokens\n",
      "Generating:  67%|██████▋   | 67/100 [04:46<01:56,  3.52s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     179.90 ms /    15 tokens (   11.99 ms per token,    83.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3680.04 ms /    47 runs   (   78.30 ms per token,    12.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    4054.68 ms /    62 tokens\n",
      "Generating:  68%|██████▊   | 68/100 [04:50<01:57,  3.69s/it]Llama.generate: 17 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     178.22 ms /    14 tokens (   12.73 ms per token,    78.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3612.99 ms /    46 runs   (   78.54 ms per token,    12.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    3984.28 ms /    60 tokens\n",
      "Generating:  69%|██████▉   | 69/100 [04:54<01:57,  3.78s/it]Llama.generate: 19 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     158.12 ms /    12 tokens (   13.18 ms per token,    75.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1906.54 ms /    24 runs   (   79.44 ms per token,    12.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    2162.11 ms /    36 tokens\n",
      "Generating:  70%|███████   | 70/100 [04:56<01:38,  3.30s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     200.54 ms /    19 tokens (   10.55 ms per token,    94.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1034.46 ms /    13 runs   (   79.57 ms per token,    12.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.47 ms /    32 tokens\n",
      "Generating:  71%|███████   | 71/100 [04:57<01:18,  2.70s/it]Llama.generate: 18 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     249.87 ms /    22 tokens (   11.36 ms per token,    88.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2449.86 ms /    31 runs   (   79.03 ms per token,    12.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    2825.58 ms /    53 tokens\n",
      "Generating:  72%|███████▏  | 72/100 [05:00<01:16,  2.74s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     260.91 ms /    24 tokens (   10.87 ms per token,    91.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2760.10 ms /    35 runs   (   78.86 ms per token,    12.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    3165.99 ms /    59 tokens\n",
      "Generating:  73%|███████▎  | 73/100 [05:03<01:17,  2.87s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     271.63 ms /    25 tokens (   10.87 ms per token,    92.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3531.35 ms /    44 runs   (   80.26 ms per token,    12.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    3985.35 ms /    69 tokens\n",
      "Generating:  74%|███████▍  | 74/100 [05:07<01:23,  3.21s/it]Llama.generate: 19 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     244.90 ms /    23 tokens (   10.65 ms per token,    93.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2945.33 ms /    37 runs   (   79.60 ms per token,    12.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    3342.21 ms /    60 tokens\n",
      "Generating:  75%|███████▌  | 75/100 [05:11<01:21,  3.25s/it]Llama.generate: 17 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     203.19 ms /    19 tokens (   10.69 ms per token,    93.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3974.24 ms /    50 runs   (   79.48 ms per token,    12.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    4388.83 ms /    69 tokens\n",
      "Generating:  76%|███████▌  | 76/100 [05:15<01:26,  3.59s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     180.35 ms /    16 tokens (   11.27 ms per token,    88.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7453.46 ms /    93 runs   (   80.14 ms per token,    12.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    8022.25 ms /   109 tokens\n",
      "Generating:  77%|███████▋  | 77/100 [05:23<01:53,  4.93s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     274.95 ms /    26 tokens (   10.57 ms per token,    94.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3247.47 ms /    41 runs   (   79.21 ms per token,    12.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    3692.40 ms /    67 tokens\n",
      "Generating:  78%|███████▊  | 78/100 [05:27<01:40,  4.56s/it]Llama.generate: 18 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     216.93 ms /    19 tokens (   11.42 ms per token,    87.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3906.30 ms /    49 runs   (   79.72 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    4331.88 ms /    68 tokens\n",
      "Generating:  79%|███████▉  | 79/100 [05:31<01:34,  4.49s/it]Llama.generate: 17 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     271.58 ms /    25 tokens (   10.86 ms per token,    92.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6722.77 ms /    85 runs   (   79.09 ms per token,    12.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    7380.78 ms /   110 tokens\n",
      "Generating:  80%|████████  | 80/100 [05:39<01:47,  5.37s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     231.58 ms /    20 tokens (   11.58 ms per token,    86.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5069.36 ms /    64 runs   (   79.21 ms per token,    12.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    5570.08 ms /    84 tokens\n",
      "Generating:  81%|████████  | 81/100 [05:44<01:43,  5.43s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     278.71 ms /    24 tokens (   11.61 ms per token,    86.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6028.83 ms /    76 runs   (   79.33 ms per token,    12.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    6627.25 ms /   100 tokens\n",
      "Generating:  82%|████████▏ | 82/100 [05:51<01:44,  5.79s/it]Llama.generate: 17 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     142.96 ms /    11 tokens (   13.00 ms per token,    76.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5129.83 ms /    65 runs   (   78.92 ms per token,    12.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    5547.80 ms /    76 tokens\n",
      "Generating:  83%|████████▎ | 83/100 [05:56<01:37,  5.72s/it]Llama.generate: 17 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     240.00 ms /    21 tokens (   11.43 ms per token,    87.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1515.28 ms /    19 runs   (   79.75 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1830.55 ms /    40 tokens\n",
      "Generating:  84%|████████▍ | 84/100 [05:58<01:12,  4.56s/it]Llama.generate: 17 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     210.44 ms /    16 tokens (   13.15 ms per token,    76.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4292.68 ms /    54 runs   (   79.49 ms per token,    12.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    4726.61 ms /    70 tokens\n",
      "Generating:  85%|████████▌ | 85/100 [06:03<01:09,  4.61s/it]Llama.generate: 17 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     222.14 ms /    15 tokens (   14.81 ms per token,    67.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.18 ms /    14 runs   (   79.58 ms per token,    12.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1394.40 ms /    29 tokens\n",
      "Generating:  86%|████████▌ | 86/100 [06:04<00:51,  3.65s/it]Llama.generate: 17 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     240.02 ms /    20 tokens (   12.00 ms per token,    83.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2042.61 ms /    26 runs   (   78.56 ms per token,    12.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    2401.17 ms /    46 tokens\n",
      "Generating:  87%|████████▋ | 87/100 [06:07<00:42,  3.28s/it]Llama.generate: 17 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     268.48 ms /    22 tokens (   12.20 ms per token,    81.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4060.52 ms /    51 runs   (   79.62 ms per token,    12.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    4540.46 ms /    73 tokens\n",
      "Generating:  88%|████████▊ | 88/100 [06:11<00:43,  3.66s/it]Llama.generate: 18 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     170.79 ms /    16 tokens (   10.67 ms per token,    93.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6794.33 ms /    85 runs   (   79.93 ms per token,    12.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    7332.43 ms /   101 tokens\n",
      "Generating:  89%|████████▉ | 89/100 [06:19<00:52,  4.77s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     321.62 ms /    23 tokens (   13.98 ms per token,    71.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =     879.97 ms /    11 runs   (   80.00 ms per token,    12.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.30 ms /    34 tokens\n",
      "Generating:  90%|█████████ | 90/100 [06:20<00:37,  3.72s/it]Llama.generate: 19 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     172.44 ms /    16 tokens (   10.78 ms per token,    92.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3819.10 ms /    48 runs   (   79.56 ms per token,    12.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    4192.56 ms /    64 tokens\n",
      "Generating:  91%|█████████ | 91/100 [06:24<00:34,  3.86s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     259.99 ms /    20 tokens (   13.00 ms per token,    76.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7765.32 ms /    98 runs   (   79.24 ms per token,    12.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    8442.03 ms /   118 tokens\n",
      "Generating:  92%|█████████▏| 92/100 [06:33<00:41,  5.24s/it]Llama.generate: 17 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     195.77 ms /    17 tokens (   11.52 ms per token,    86.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2118.05 ms /    27 runs   (   78.45 ms per token,    12.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    2422.66 ms /    44 tokens\n",
      "Generating:  93%|█████████▎| 93/100 [06:35<00:30,  4.40s/it]Llama.generate: 18 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     281.01 ms /    20 tokens (   14.05 ms per token,    71.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6337.85 ms /    80 runs   (   79.22 ms per token,    12.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    6956.16 ms /   100 tokens\n",
      "Generating:  94%|█████████▍| 94/100 [06:42<00:31,  5.17s/it]Llama.generate: 17 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     255.41 ms /    23 tokens (   11.10 ms per token,    90.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3814.77 ms /    49 runs   (   77.85 ms per token,    12.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    4270.83 ms /    72 tokens\n",
      "Generating:  95%|█████████▌| 95/100 [06:46<00:24,  4.90s/it]Llama.generate: 17 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     313.43 ms /    26 tokens (   12.05 ms per token,    82.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3479.91 ms /    44 runs   (   79.09 ms per token,    12.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    3982.82 ms /    70 tokens\n",
      "Generating:  96%|█████████▌| 96/100 [06:50<00:18,  4.63s/it]Llama.generate: 18 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     150.80 ms /    12 tokens (   12.57 ms per token,    79.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5146.41 ms /    65 runs   (   79.18 ms per token,    12.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    5576.86 ms /    77 tokens\n",
      "Generating:  97%|█████████▋| 97/100 [06:56<00:14,  4.92s/it]Llama.generate: 19 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     278.27 ms /    25 tokens (   11.13 ms per token,    89.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5063.36 ms /    64 runs   (   79.11 ms per token,    12.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    5609.49 ms /    89 tokens\n",
      "Generating:  98%|█████████▊| 98/100 [07:02<00:10,  5.13s/it]Llama.generate: 17 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     263.44 ms /    24 tokens (   10.98 ms per token,    91.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1733.68 ms /    22 runs   (   78.80 ms per token,    12.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    2091.83 ms /    46 tokens\n",
      "Generating:  99%|█████████▉| 99/100 [07:04<00:04,  4.22s/it]Llama.generate: 18 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     375.27 ms\n",
      "llama_perf_context_print: prompt eval time =     135.67 ms /    10 tokens (   13.57 ms per token,    73.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3837.12 ms /    49 runs   (   78.31 ms per token,    12.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    4177.20 ms /    59 tokens\n",
      "Generating: 100%|██████████| 100/100 [07:08<00:00,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for model  Llama-3.2-1B_FT_q8_0.gguf\n",
      "Total 100 examples,  428.3s\n",
      "Mean latency      4.280s\n",
      "Mean throughput   11.47 tok/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logs = genDataset(llm, evaluate_df, CUSTOM_q8_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Wi7Wi5QrEDK1",
   "metadata": {
    "id": "Wi7Wi5QrEDK1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 30.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8.27 seconds, 12.08 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Baseline not Found for bert-base-multilingual-cased on ru at C:\\Tools\\anaconda3\\envs\\llm_in_finance\\lib\\site-packages\\bert_score\\rescale_baseline/ru/bert-base-multilingual-cased.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>pred</th>\n",
       "      <th>ref</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>gen_tokens</th>\n",
       "      <th>latency_sec</th>\n",
       "      <th>tok_per_sec</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Тот, кто уплачивает налог.</td>\n",
       "      <td>Налогоплательщик — это физическое или юридичес...</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>1.219093</td>\n",
       "      <td>9.023100</td>\n",
       "      <td>0.731216</td>\n",
       "      <td>0.621312</td>\n",
       "      <td>0.671799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Далее необходимо заполнить налоговые деклараци...</td>\n",
       "      <td>Налоговое обязательство включает в себя обязан...</td>\n",
       "      <td>31</td>\n",
       "      <td>84</td>\n",
       "      <td>7.234641</td>\n",
       "      <td>11.610805</td>\n",
       "      <td>0.722255</td>\n",
       "      <td>0.748582</td>\n",
       "      <td>0.735183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Налогоплательщик имеет право подать отчетность...</td>\n",
       "      <td>Налогоплательщик имеет право получать разъясне...</td>\n",
       "      <td>30</td>\n",
       "      <td>58</td>\n",
       "      <td>4.968583</td>\n",
       "      <td>11.673348</td>\n",
       "      <td>0.771635</td>\n",
       "      <td>0.784500</td>\n",
       "      <td>0.778014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Налогоплательщик обязан предоставить налоговом...</td>\n",
       "      <td>Налогоплательщик обязан своевременно и в полно...</td>\n",
       "      <td>35</td>\n",
       "      <td>50</td>\n",
       "      <td>4.506871</td>\n",
       "      <td>11.094170</td>\n",
       "      <td>0.782600</td>\n",
       "      <td>0.754486</td>\n",
       "      <td>0.768286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Налог на прибыль, НДС (налог на продажу), нало...</td>\n",
       "      <td>Налоговым кодексом установлены налоги на доход...</td>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "      <td>3.205061</td>\n",
       "      <td>10.296216</td>\n",
       "      <td>0.794666</td>\n",
       "      <td>0.751557</td>\n",
       "      <td>0.772510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                               pred  \\\n",
       "0    0                         Тот, кто уплачивает налог.   \n",
       "1    1  Далее необходимо заполнить налоговые деклараци...   \n",
       "2    2  Налогоплательщик имеет право подать отчетность...   \n",
       "3    3  Налогоплательщик обязан предоставить налоговом...   \n",
       "4    4  Налог на прибыль, НДС (налог на продажу), нало...   \n",
       "\n",
       "                                                 ref  prompt_tokens  \\\n",
       "0  Налогоплательщик — это физическое или юридичес...             28   \n",
       "1  Налоговое обязательство включает в себя обязан...             31   \n",
       "2  Налогоплательщик имеет право получать разъясне...             30   \n",
       "3  Налогоплательщик обязан своевременно и в полно...             35   \n",
       "4  Налоговым кодексом установлены налоги на доход...             39   \n",
       "\n",
       "   gen_tokens  latency_sec  tok_per_sec         P         R        F1  \n",
       "0          11     1.219093     9.023100  0.731216  0.621312  0.671799  \n",
       "1          84     7.234641    11.610805  0.722255  0.748582  0.735183  \n",
       "2          58     4.968583    11.673348  0.771635  0.784500  0.778014  \n",
       "3          50     4.506871    11.094170  0.782600  0.754486  0.768286  \n",
       "4          33     3.205061    10.296216  0.794666  0.751557  0.772510  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean BERTScore  P=0.7142  R=0.7114  F1=0.7122\n"
     ]
    }
   ],
   "source": [
    "logs = pd.read_csv(\"C:/Users/csode/project/evaluate/generated_responses_Llama-3.2-1B_FT_q8_0.gguf.csv\")\n",
    "evaluateBERTScore(CUSTOM_q8_0, logs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
